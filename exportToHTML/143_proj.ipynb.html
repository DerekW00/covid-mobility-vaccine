<html>
<head>
<title>143_proj.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #8c8c8c; font-style: italic;}
.s1 { color: #0033b3;}
.s2 { color: #080808;}
.s3 { color: #067d17;}
.s4 { color: #1750eb;}
.s5 { color: #0037a6;}
.ln { color: #adadad; font-weight: normal; font-style: normal; }
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
143_proj.ipynb</font>
</center></td></tr></table>
<pre><a name="l1"><span class="ln">1    </span></a><span class="s0">#%% 
<a name="l2"><span class="ln">2    </span></a></span><span class="s1">import </span><span class="s2">numpy </span><span class="s1">as </span><span class="s2">np</span>
<a name="l3"><span class="ln">3    </span></a><span class="s1">import </span><span class="s2">pandas </span><span class="s1">as </span><span class="s2">pd</span>
<a name="l4"><span class="ln">4    </span></a><span class="s1">import </span><span class="s2">scipy.stats </span><span class="s1">as </span><span class="s2">stats</span>
<a name="l5"><span class="ln">5    </span></a><span class="s1">import </span><span class="s2">matplotlib.patches </span><span class="s1">as </span><span class="s2">patches</span>
<a name="l6"><span class="ln">6    </span></a><span class="s1">import </span><span class="s2">matplotlib.pyplot </span><span class="s1">as </span><span class="s2">plt</span>
<a name="l7"><span class="ln">7    </span></a><span class="s1">import </span><span class="s2">seaborn </span><span class="s1">as </span><span class="s2">sns</span>
<a name="l8"><span class="ln">8    </span></a><span class="s1">import </span><span class="s2">plotly.figure_factory </span><span class="s1">as </span><span class="s2">ff</span>
<a name="l9"><span class="ln">9    </span></a><span class="s1">import </span><span class="s2">plotly.express </span><span class="s1">as </span><span class="s2">px</span>
<a name="l10"><span class="ln">10   </span></a>
<a name="l11"><span class="ln">11   </span></a><span class="s1">import </span><span class="s2">itertools</span>
<a name="l12"><span class="ln">12   </span></a><span class="s1">import </span><span class="s2">statsmodels.api </span><span class="s1">as </span><span class="s2">sm</span>
<a name="l13"><span class="ln">13   </span></a><span class="s1">import </span><span class="s2">datetime</span>
<a name="l14"><span class="ln">14   </span></a><span class="s1">import </span><span class="s2">geopandas </span><span class="s1">as </span><span class="s2">gpd</span>
<a name="l15"><span class="ln">15   </span></a><span class="s1">import </span><span class="s2">warnings</span>
<a name="l16"><span class="ln">16   </span></a><span class="s1">import </span><span class="s2">array_to_latex </span><span class="s1">as </span><span class="s2">a2l</span>
<a name="l17"><span class="ln">17   </span></a>
<a name="l18"><span class="ln">18   </span></a><span class="s2">warnings.filterwarnings(</span><span class="s3">'ignore'</span><span class="s2">)</span>
<a name="l19"><span class="ln">19   </span></a><span class="s0">#%% md 
<a name="l20"><span class="ln">20   </span></a></span><span class="s2"># COVID, Mobility, and Vaccination in California 
<a name="l21"><span class="ln">21   </span></a>**Spring 2023 Econ 143 Final Project** 
<a name="l22"><span class="ln">22   </span></a>Prepared by: **Derek Derui Wang** 
<a name="l23"><span class="ln">23   </span></a>## Overview 
<a name="l24"><span class="ln">24   </span></a>### Objective 
<a name="l25"><span class="ln">25   </span></a>The primary objective of this research project is to understand 
<a name="l26"><span class="ln">26   </span></a>the relationship between mobility, COVID infection, and death rates. 
<a name="l27"><span class="ln">27   </span></a> 
<a name="l28"><span class="ln">28   </span></a>[Glaser, Gorback, and Redding (2020)] (https://scholar.harvard.edu/glaeser/publications/how-much-does-covid-19-increase-mobility-evidence-new-york-and-four-other-us) find substantial spatial and temporal heterogeneity; east coast cities have stronger effects, with the largest for NYC in the pandemicâ€™s early stages by instrument for travel by residential teleworkable and essential shares and panel data for NYC with week and zip code. However, this working paper used data from the beginning of the pandemic in 2020. In this project, I will perform a similar analysis using data on the FIPS level. I will add demographic data, including unemployment rate, population density, political affiliation, poverty, and education level, to explore if these commonly believed factors affect the COVID infection and Death. 
<a name="l29"><span class="ln">29   </span></a> 
<a name="l30"><span class="ln">30   </span></a>### Selection of States 
<a name="l31"><span class="ln">31   </span></a>After performing EDA, I selected eight states with the least missing data based on the percentage of missing and the number of missings. The primary focus will be on New York, Maryland, and Pennsylvania because they outperformed on both metrics. I added four more states,  California, Maine,  Ohio, and South Carolina, as supplementary to aid and confirm the discovery. 
<a name="l32"><span class="ln">32   </span></a> 
<a name="l33"><span class="ln">33   </span></a>## Data 
<a name="l34"><span class="ln">34   </span></a>### Data Source and Merge 
<a name="l35"><span class="ln">35   </span></a>#### [Google Community Mobility Reports](https://www.google.com/covid19/mobility/) 
<a name="l36"><span class="ln">36   </span></a>The dataset comprises the location or region and fluctuations in visits to places like grocery stores, parks, 
<a name="l37"><span class="ln">37   </span></a> transit stations, and workplaces compared to the baseline. The baseline day represents the median 
<a name="l38"><span class="ln">38   </span></a> value from the five weeks between January 3 and February 6, 2020. The data constitute a census, 
<a name="l39"><span class="ln">39   </span></a> but I recognize the technological constraints during the data collection period. The mobility 
<a name="l40"><span class="ln">40   </span></a> information 
<a name="l41"><span class="ln">41   </span></a> relies on the location services of each individual's phone. Consequently, the dataset will 
<a name="l42"><span class="ln">42   </span></a> systematically omit individuals who did not install Google's apps on their phones or 
<a name="l43"><span class="ln">43   </span></a> did not allow Google's apps to access their locations. Similar limitations are highlighted 
<a name="l44"><span class="ln">44   </span></a> in the article about a Berlin artist exploiting this issue to trick Google into a traffic jam alert from [the Guardian](https://www.theguardian.com/technology/2020/feb/03/berlin-artist-uses-99-phones-trick-google-maps-traffic-jam-alert). 
<a name="l45"><span class="ln">45   </span></a> 
<a name="l46"><span class="ln">46   </span></a>The data's granularity is daily at the county (FIPS) level. Compared to the baseline, each row signifies the changes in visits to public places, such as transit stations, parks, and workplaces. It is crucial to consider this dataset's features when examining mobility trends and their connections with demographic information like infection and vaccination rates. This is because these changes require time to impact mobility, making assessing changes over a more extended period essential. 
<a name="l47"><span class="ln">47   </span></a> 
<a name="l48"><span class="ln">48   </span></a>A significant concern I have about the dataset is selection bias. The data is gathered through Google's Apps on individuals' smartphones, which excludes those who do not possess smartphones. The data depends on the smartphones' location and necessitates permission for the collection, systematically excluding groups that denied the app access to their location for various reasons. 
<a name="l49"><span class="ln">49   </span></a></span><span class="s0">#%% 
<a name="l50"><span class="ln">50   </span></a></span><span class="s2">df_2022 = pd.read_csv(</span><span class="s3">&quot;input/2022_US_Region_Mobility_Report.csv&quot;</span><span class="s2">, dtype={</span><span class="s3">'census_fips_code'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l51"><span class="ln">51   </span></a><span class="s2">df_2021 = pd.read_csv(</span><span class="s3">&quot;input/2021_US_Region_Mobility_Report.csv&quot;</span><span class="s2">, dtype={</span><span class="s3">'census_fips_code'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l52"><span class="ln">52   </span></a><span class="s2">df_2020 = pd.read_csv(</span><span class="s3">'input/2020_US_Region_Mobility_Report.csv'</span><span class="s2">, dtype={</span><span class="s3">'census_fips_code'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l53"><span class="ln">53   </span></a><span class="s2">mobility_df = pd.concat([df_2020, df_2021, df_2022])</span>
<a name="l54"><span class="ln">54   </span></a><span class="s2">mobility_df.rename(columns={</span><span class="s3">'retail_and_recreation_percent_change_from_baseline'</span><span class="s2">: </span><span class="s3">'retail_and_recreation'</span><span class="s2">,</span>
<a name="l55"><span class="ln">55   </span></a>                            <span class="s3">'grocery_and_pharmacy_percent_change_from_baseline'</span><span class="s2">: </span><span class="s3">'grocery_and_pharmacy'</span><span class="s2">,</span>
<a name="l56"><span class="ln">56   </span></a>                            <span class="s3">'parks_percent_change_from_baseline'</span><span class="s2">: </span><span class="s3">'parks'</span><span class="s2">,</span>
<a name="l57"><span class="ln">57   </span></a>                            <span class="s3">'transit_stations_percent_change_from_baseline'</span><span class="s2">: </span><span class="s3">'transit_stations'</span><span class="s2">,</span>
<a name="l58"><span class="ln">58   </span></a>                            <span class="s3">'workplaces_percent_change_from_baseline'</span><span class="s2">: </span><span class="s3">'workplaces'</span><span class="s2">,</span>
<a name="l59"><span class="ln">59   </span></a>                            <span class="s3">'residential_percent_change_from_baseline'</span><span class="s2">: </span><span class="s3">'residential'</span><span class="s2">,</span>
<a name="l60"><span class="ln">60   </span></a>                            <span class="s3">'sub_region_1'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'sub_region_2'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'census_fips_code'</span><span class="s2">: </span><span class="s3">'fips'</span><span class="s2">},</span>
<a name="l61"><span class="ln">61   </span></a>                   <span class="s2">inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l62"><span class="ln">62   </span></a><span class="s2">mobility_df.drop(columns=[</span><span class="s3">'country_region_code'</span><span class="s2">, </span><span class="s3">'country_region'</span><span class="s2">, </span><span class="s3">'metro_area'</span><span class="s2">, </span><span class="s3">'iso_3166_2_code'</span><span class="s2">, </span><span class="s3">'place_id'</span><span class="s2">],</span>
<a name="l63"><span class="ln">63   </span></a>                 <span class="s2">inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l64"><span class="ln">64   </span></a><span class="s2">mobility_df.dropna(subset=[</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l65"><span class="ln">65   </span></a><span class="s2">mobility_df.reset_index(drop=</span><span class="s1">True</span><span class="s2">, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l66"><span class="ln">66   </span></a><span class="s2">mobility_df[</span><span class="s3">'date'</span><span class="s2">] = pd.to_datetime(mobility_df[</span><span class="s3">'date'</span><span class="s2">])</span>
<a name="l67"><span class="ln">67   </span></a><span class="s0">#%% md 
<a name="l68"><span class="ln">68   </span></a></span><span class="s2">#### [COVID Cases and Deaths: New York Times](https://github.com/nytimes/covid-19-data) 
<a name="l69"><span class="ln">69   </span></a></span><span class="s0">#%% 
<a name="l70"><span class="ln">70   </span></a></span><span class="s2">df_cases_2020 = pd.read_csv(</span><span class="s3">'input/us-counties-2020.csv'</span><span class="s2">, dtype={</span><span class="s3">'date'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l71"><span class="ln">71   </span></a><span class="s2">df_cases_2021 = pd.read_csv(</span><span class="s3">'input/us-counties-2021.csv'</span><span class="s2">, dtype={</span><span class="s3">'date'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l72"><span class="ln">72   </span></a><span class="s2">df_cases_2022 = pd.read_csv(</span><span class="s3">'input/us-counties-2022.csv'</span><span class="s2">, dtype={</span><span class="s3">'date'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l73"><span class="ln">73   </span></a><span class="s2">cases_df = pd.concat([df_cases_2020, df_cases_2021, df_cases_2022])</span>
<a name="l74"><span class="ln">74   </span></a><span class="s2">cases_df[</span><span class="s3">'date'</span><span class="s2">] = pd.to_datetime(cases_df[</span><span class="s3">'date'</span><span class="s2">])</span>
<a name="l75"><span class="ln">75   </span></a><span class="s2">cases_df.dropna(subset=[</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l76"><span class="ln">76   </span></a><span class="s2">cases_df.reset_index(drop=</span><span class="s1">True</span><span class="s2">, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l77"><span class="ln">77   </span></a><span class="s2">cases_df[</span><span class="s3">'daily_cases'</span><span class="s2">] = cases_df.sort_values(</span><span class="s3">'date'</span><span class="s2">).groupby(</span><span class="s3">'fips'</span><span class="s2">)[</span><span class="s3">'cases'</span><span class="s2">].diff().fillna(</span>
<a name="l78"><span class="ln">78   </span></a>    <span class="s2">cases_df[</span><span class="s3">'cases'</span><span class="s2">])  </span><span class="s0">#Daily cases from cumulative cases</span>
<a name="l79"><span class="ln">79   </span></a><span class="s2">cases_df[</span><span class="s3">'daily_deaths'</span><span class="s2">] = cases_df.sort_values(</span><span class="s3">'date'</span><span class="s2">).groupby(</span><span class="s3">'fips'</span><span class="s2">)[</span><span class="s3">'deaths'</span><span class="s2">].diff().fillna(</span>
<a name="l80"><span class="ln">80   </span></a>    <span class="s2">cases_df[</span><span class="s3">'deaths'</span><span class="s2">])  </span><span class="s0">#Daily deaths from cumulative deaths</span>
<a name="l81"><span class="ln">81   </span></a><span class="s0">#%% 
<a name="l82"><span class="ln">82   </span></a></span><span class="s2">df = cases_df.merge(mobility_df, on=[</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">], how=</span><span class="s3">'inner'</span><span class="s2">)</span>
<a name="l83"><span class="ln">83   </span></a><span class="s2">df.drop(columns=[</span><span class="s3">'county_y'</span><span class="s2">, </span><span class="s3">'state_y'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l84"><span class="ln">84   </span></a><span class="s2">df.rename(columns={</span><span class="s3">'county_x'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'state_x'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l85"><span class="ln">85   </span></a><span class="s2">df.head(</span><span class="s4">5</span><span class="s2">)</span>
<a name="l86"><span class="ln">86   </span></a><span class="s0">#%% md 
<a name="l87"><span class="ln">87   </span></a></span><span class="s2">#### Population and Land Area Data, FIPS level by the United States Environmental Protection Agency 
<a name="l88"><span class="ln">88   </span></a>This [population dataset](https://www.epa.gov/sites/default/files/2016-04/ozone-county-population.xlsx) includes the population and land area on FIPS level. The population density is calculated by dividing the population by the land area. The dataset is from the United States Environmental Protection Agency. 
<a name="l89"><span class="ln">89   </span></a></span><span class="s0">#%% 
<a name="l90"><span class="ln">90   </span></a></span><span class="s2">population = pd.read_excel(</span><span class="s3">'input/ozone-county-population.xlsx'</span><span class="s2">,</span>
<a name="l91"><span class="ln">91   </span></a>                           <span class="s2">dtype={</span><span class="s3">'STATE FIPS'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'COUNTY FIPS'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">' 2015 POPULATION'</span><span class="s2">: </span><span class="s3">'float'</span><span class="s2">,</span>
<a name="l92"><span class="ln">92   </span></a>                                  <span class="s3">' LAND AREA (Sqare Miles)'</span><span class="s2">: </span><span class="s3">'float'</span><span class="s2">})</span>
<a name="l93"><span class="ln">93   </span></a><span class="s2">population[</span><span class="s3">'fips'</span><span class="s2">] = population[</span><span class="s3">'STATE FIPS'</span><span class="s2">] + population[</span><span class="s3">'COUNTY FIPS'</span><span class="s2">]</span>
<a name="l94"><span class="ln">94   </span></a><span class="s2">population.drop(columns=[</span><span class="s3">'STATE FIPS'</span><span class="s2">, </span><span class="s3">'COUNTY FIPS'</span><span class="s2">, </span><span class="s3">'2010 POPULATION'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l95"><span class="ln">95   </span></a><span class="s2">population.rename(columns={</span><span class="s3">'2015 POPULATION'</span><span class="s2">: </span><span class="s3">'population'</span><span class="s2">, </span><span class="s3">'LAND AREA (Sqare Miles)'</span><span class="s2">: </span><span class="s3">'land_area'</span><span class="s2">,</span>
<a name="l96"><span class="ln">96   </span></a>                           <span class="s3">'STATE/TERRITORY NAME'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'COUNTY NAME'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l97"><span class="ln">97   </span></a><span class="s2">population[</span><span class="s3">'population_density'</span><span class="s2">] = population[</span><span class="s3">'population'</span><span class="s2">] / population[</span><span class="s3">'land_area'</span><span class="s2">]</span>
<a name="l98"><span class="ln">98   </span></a><span class="s2">population.head(</span><span class="s4">5</span><span class="s2">)</span>
<a name="l99"><span class="ln">99   </span></a><span class="s0">#%% md 
<a name="l100"><span class="ln">100  </span></a></span><span class="s2">#### Political Party Affiliations in the US by state 
<a name="l101"><span class="ln">101  </span></a>The MIT Election Data and Science Lab provides a [dataset](https://doi.org/10.7910/DVN/VOQCHQ) that includes the political party affiliations of each county. The dataset is from the MIT Election Data and Science Lab. 
<a name="l102"><span class="ln">102  </span></a></span><span class="s0">#%% 
<a name="l103"><span class="ln">103  </span></a></span><span class="s2">political_df = pd.read_csv(</span><span class="s3">'input/countypres_2000-2020.csv'</span><span class="s2">, dtype={</span><span class="s3">'county_fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l104"><span class="ln">104  </span></a><span class="s2">political_df = political_df[political_df[</span><span class="s3">'year'</span><span class="s2">] == </span><span class="s4">2020</span><span class="s2">]</span>
<a name="l105"><span class="ln">105  </span></a><span class="s2">political_df.drop(columns=[</span><span class="s3">'year'</span><span class="s2">, </span><span class="s3">'office'</span><span class="s2">, </span><span class="s3">'version'</span><span class="s2">, </span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'candidate'</span><span class="s2">, </span><span class="s3">'candidatevotes'</span><span class="s2">, </span><span class="s3">'totalvotes'</span><span class="s2">, </span><span class="s3">'mode'</span><span class="s2">],</span>
<a name="l106"><span class="ln">106  </span></a>                  <span class="s2">inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l107"><span class="ln">107  </span></a><span class="s2">political_df.rename(columns={</span><span class="s3">'county_fips'</span><span class="s2">: </span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'county_name'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'state_po'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l108"><span class="ln">108  </span></a><span class="s2">df = df.merge(political_df, on=[</span><span class="s3">'fips'</span><span class="s2">], how=</span><span class="s3">'left'</span><span class="s2">)</span>
<a name="l109"><span class="ln">109  </span></a><span class="s2">df.drop(columns=[</span><span class="s3">'county_y'</span><span class="s2">, </span><span class="s3">'state_y'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l110"><span class="ln">110  </span></a><span class="s2">df.rename(columns={</span><span class="s3">'county_x'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'state_x'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l111"><span class="ln">111  </span></a><span class="s2">df.head(</span><span class="s4">5</span><span class="s2">)</span>
<a name="l112"><span class="ln">112  </span></a><span class="s0">#%% md 
<a name="l113"><span class="ln">113  </span></a></span><span class="s2">#### [Local Area Unemployment Statistics by U.S. Bureau of Labor Statistics](https://www.bls.gov/web/laus/laumstrk.htm) 
<a name="l114"><span class="ln">114  </span></a></span><span class="s0">#%% 
<a name="l115"><span class="ln">115  </span></a></span><span class="s2">unemployment_2020 = pd.read_excel(</span><span class="s3">'input/laucnty20.xlsx'</span><span class="s2">, dtype={</span><span class="s3">'state_fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'county_fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l116"><span class="ln">116  </span></a><span class="s2">unemployment_2020[</span><span class="s3">'fips'</span><span class="s2">] = unemployment_2020[</span><span class="s3">'state_fips'</span><span class="s2">] + unemployment_2020[</span><span class="s3">'county_fips'</span><span class="s2">]</span>
<a name="l117"><span class="ln">117  </span></a>
<a name="l118"><span class="ln">118  </span></a><span class="s2">unemployment_2020 = unemployment_2020[unemployment_2020[</span><span class="s3">'unemployment_rate'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l119"><span class="ln">119  </span></a><span class="s2">unemployment_2020 = unemployment_2020[unemployment_2020[</span><span class="s3">'unemployed'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l120"><span class="ln">120  </span></a><span class="s2">unemployment_2020 = unemployment_2020[unemployment_2020[</span><span class="s3">'employed'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l121"><span class="ln">121  </span></a><span class="s2">unemployment_2020 = unemployment_2020[unemployment_2020[</span><span class="s3">'labor_force'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l122"><span class="ln">122  </span></a><span class="s2">unemployment_2020[</span><span class="s3">'labor_force'</span><span class="s2">] = unemployment_2020[</span><span class="s3">'labor_force'</span><span class="s2">].astype(float)</span>
<a name="l123"><span class="ln">123  </span></a><span class="s2">unemployment_2020[</span><span class="s3">'unemployment_rate'</span><span class="s2">] = unemployment_2020[</span><span class="s3">'unemployment_rate'</span><span class="s2">].astype(float)</span>
<a name="l124"><span class="ln">124  </span></a><span class="s2">unemployment_2020[</span><span class="s3">'unemployed'</span><span class="s2">] = unemployment_2020[</span><span class="s3">'unemployed'</span><span class="s2">].astype(float)</span>
<a name="l125"><span class="ln">125  </span></a><span class="s2">unemployment_2020[</span><span class="s3">'employed'</span><span class="s2">] = unemployment_2020[</span><span class="s3">'employed'</span><span class="s2">].astype(float)</span>
<a name="l126"><span class="ln">126  </span></a>
<a name="l127"><span class="ln">127  </span></a><span class="s2">unemployment_2021 = pd.read_excel(</span><span class="s3">'input/laucnty21.xlsx'</span><span class="s2">, dtype={</span><span class="s3">'state_fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'county_fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l128"><span class="ln">128  </span></a><span class="s2">unemployment_2021[</span><span class="s3">'fips'</span><span class="s2">] = unemployment_2021[</span><span class="s3">'state_fips'</span><span class="s2">] + unemployment_2021[</span><span class="s3">'county_fips'</span><span class="s2">]</span>
<a name="l129"><span class="ln">129  </span></a>
<a name="l130"><span class="ln">130  </span></a><span class="s2">unemployment_2021 = unemployment_2021[unemployment_2021[</span><span class="s3">'unemployment_rate'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l131"><span class="ln">131  </span></a><span class="s2">unemployment_2021 = unemployment_2021[unemployment_2021[</span><span class="s3">'unemployed'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l132"><span class="ln">132  </span></a><span class="s2">unemployment_2021 = unemployment_2021[unemployment_2021[</span><span class="s3">'employed'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l133"><span class="ln">133  </span></a><span class="s2">unemployment_2021 = unemployment_2021[unemployment_2021[</span><span class="s3">'labor_force'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l134"><span class="ln">134  </span></a><span class="s2">unemployment_2021[</span><span class="s3">'labor_force'</span><span class="s2">] = unemployment_2021[</span><span class="s3">'labor_force'</span><span class="s2">].astype(float)</span>
<a name="l135"><span class="ln">135  </span></a><span class="s2">unemployment_2021[</span><span class="s3">'unemployment_rate'</span><span class="s2">] = unemployment_2021[</span><span class="s3">'unemployment_rate'</span><span class="s2">].astype(float)</span>
<a name="l136"><span class="ln">136  </span></a><span class="s2">unemployment_2021[</span><span class="s3">'unemployed'</span><span class="s2">] = unemployment_2021[</span><span class="s3">'unemployed'</span><span class="s2">].astype(float)</span>
<a name="l137"><span class="ln">137  </span></a><span class="s2">unemployment_2021[</span><span class="s3">'employed'</span><span class="s2">] = unemployment_2021[</span><span class="s3">'employed'</span><span class="s2">].astype(float)</span>
<a name="l138"><span class="ln">138  </span></a>
<a name="l139"><span class="ln">139  </span></a><span class="s2">unemployment_2022 = pd.read_excel(</span><span class="s3">'input/laucnty22.xlsx'</span><span class="s2">, dtype={</span><span class="s3">'state_fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">, </span><span class="s3">'county_fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l140"><span class="ln">140  </span></a><span class="s2">unemployment_2022[</span><span class="s3">'fips'</span><span class="s2">] = unemployment_2022[</span><span class="s3">'state_fips'</span><span class="s2">] + unemployment_2022[</span><span class="s3">'county_fips'</span><span class="s2">]</span>
<a name="l141"><span class="ln">141  </span></a>
<a name="l142"><span class="ln">142  </span></a><span class="s2">unemployment_2022 = unemployment_2022[unemployment_2022[</span><span class="s3">'unemployment_rate'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l143"><span class="ln">143  </span></a><span class="s2">unemployment_2022 = unemployment_2022[unemployment_2022[</span><span class="s3">'unemployed'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l144"><span class="ln">144  </span></a><span class="s2">unemployment_2022 = unemployment_2022[unemployment_2022[</span><span class="s3">'employed'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l145"><span class="ln">145  </span></a><span class="s2">unemployment_2022 = unemployment_2022[unemployment_2022[</span><span class="s3">'labor_force'</span><span class="s2">] != </span><span class="s3">'N.A.'</span><span class="s2">]</span>
<a name="l146"><span class="ln">146  </span></a><span class="s2">unemployment_2022[</span><span class="s3">'labor_force'</span><span class="s2">] = unemployment_2022[</span><span class="s3">'labor_force'</span><span class="s2">].astype(float)</span>
<a name="l147"><span class="ln">147  </span></a><span class="s2">unemployment_2022[</span><span class="s3">'unemployment_rate'</span><span class="s2">] = unemployment_2022[</span><span class="s3">'unemployment_rate'</span><span class="s2">].astype(float)</span>
<a name="l148"><span class="ln">148  </span></a><span class="s2">unemployment_2022[</span><span class="s3">'unemployed'</span><span class="s2">] = unemployment_2022[</span><span class="s3">'unemployed'</span><span class="s2">].astype(float)</span>
<a name="l149"><span class="ln">149  </span></a><span class="s2">unemployment_2022[</span><span class="s3">'employed'</span><span class="s2">] = unemployment_2022[</span><span class="s3">'employed'</span><span class="s2">].astype(float)</span>
<a name="l150"><span class="ln">150  </span></a><span class="s2">unemployment_2022.drop(columns=[</span><span class="s3">'county, state'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l151"><span class="ln">151  </span></a><span class="s2">unemployment_df = pd.concat([unemployment_2020, unemployment_2021, unemployment_2022])</span>
<a name="l152"><span class="ln">152  </span></a><span class="s2">unemployment_df.head(</span><span class="s4">5</span><span class="s2">)</span>
<a name="l153"><span class="ln">153  </span></a><span class="s0">#%% 
<a name="l154"><span class="ln">154  </span></a></span><span class="s2">df = df.merge(population, on=[</span><span class="s3">'fips'</span><span class="s2">], how=</span><span class="s3">'left'</span><span class="s2">)</span>
<a name="l155"><span class="ln">155  </span></a><span class="s2">df.drop(columns=[</span><span class="s3">'county_y'</span><span class="s2">, </span><span class="s3">'state_y'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l156"><span class="ln">156  </span></a><span class="s2">df.rename(columns={</span><span class="s3">'county_x'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'state_x'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l157"><span class="ln">157  </span></a><span class="s2">df[</span><span class="s3">'year'</span><span class="s2">] = df[</span><span class="s3">'date'</span><span class="s2">].dt.year</span>
<a name="l158"><span class="ln">158  </span></a><span class="s2">df = df.merge(unemployment_df, on=[</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'year'</span><span class="s2">], how=</span><span class="s3">'left'</span><span class="s2">)</span>
<a name="l159"><span class="ln">159  </span></a><span class="s0">#df.drop(columns=['county_y', 'state_y'], inplace=True)</span>
<a name="l160"><span class="ln">160  </span></a><span class="s2">df.rename(columns={</span><span class="s3">'county_x'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'state_x'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l161"><span class="ln">161  </span></a><span class="s2">df.head(</span><span class="s4">5</span><span class="s2">)</span>
<a name="l162"><span class="ln">162  </span></a><span class="s0">#%% md 
<a name="l163"><span class="ln">163  </span></a></span><span class="s2">#### Education Level 
<a name="l164"><span class="ln">164  </span></a>The [education level dataset](https://www.ers.usda.gov/data-products/county-level-data-sets/download-data.aspx) includes the education level by county. We are only focusing on the education level in 2017-2021. 
<a name="l165"><span class="ln">165  </span></a></span><span class="s0">#%% 
<a name="l166"><span class="ln">166  </span></a></span><span class="s2">education_df = pd.read_excel(</span><span class="s3">'input/Education.xlsx'</span><span class="s2">, dtype={</span><span class="s3">'fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l167"><span class="ln">167  </span></a><span class="s2">education_df.rename(</span>
<a name="l168"><span class="ln">168  </span></a>    <span class="s2">columns={</span><span class="s3">'Percent of adults completing some college or associate</span><span class="s5">\'</span><span class="s3">s degree, 2017-21'</span><span class="s2">: </span><span class="s3">'some_college'</span><span class="s2">,</span>
<a name="l169"><span class="ln">169  </span></a>             <span class="s3">'Percent of adults with a bachelor</span><span class="s5">\'</span><span class="s3">s degree or higher, 2017-21'</span><span class="s2">: </span><span class="s3">'bachelor_or_higher'</span><span class="s2">,</span>
<a name="l170"><span class="ln">170  </span></a>             <span class="s3">'Percent of adults with a high school diploma only, 2017-21'</span><span class="s2">: </span><span class="s3">'high_school'</span><span class="s2">,</span>
<a name="l171"><span class="ln">171  </span></a>             <span class="s3">'Percent of adults with less than a high school diploma, 2017-21'</span><span class="s2">: </span><span class="s3">'less_than_high_school'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l172"><span class="ln">172  </span></a><span class="s2">education_df = education_df[[</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'some_college'</span><span class="s2">, </span><span class="s3">'bachelor_or_higher'</span><span class="s2">, </span><span class="s3">'high_school'</span><span class="s2">, </span><span class="s3">'less_than_high_school'</span><span class="s2">]]</span>
<a name="l173"><span class="ln">173  </span></a><span class="s2">df = df.merge(education_df, on=[</span><span class="s3">'fips'</span><span class="s2">], how=</span><span class="s3">'left'</span><span class="s2">)</span>
<a name="l174"><span class="ln">174  </span></a><span class="s0">#df.drop(columns=['county_y', 'state_y'], inplace=True)</span>
<a name="l175"><span class="ln">175  </span></a><span class="s2">df.rename(columns={</span><span class="s3">'county_x'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'state_x'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l176"><span class="ln">176  </span></a><span class="s2">df.head(</span><span class="s4">5</span><span class="s2">)</span>
<a name="l177"><span class="ln">177  </span></a><span class="s0">#%% md 
<a name="l178"><span class="ln">178  </span></a></span><span class="s2">#### Poverty Rate 
<a name="l179"><span class="ln">179  </span></a>The [poverty rate dataset](https://www.ers.usda.gov/data-products/county-level-data-sets/download-data.aspx) includes the poverty rate by county. We are only focusing on the poverty rate in 2019. 
<a name="l180"><span class="ln">180  </span></a></span><span class="s0">#%% 
<a name="l181"><span class="ln">181  </span></a></span><span class="s2">poverty = pd.read_excel(</span><span class="s3">'input/PovertyEstimates.xlsx'</span><span class="s2">, dtype={</span><span class="s3">'fips'</span><span class="s2">: </span><span class="s3">'str'</span><span class="s2">})</span>
<a name="l182"><span class="ln">182  </span></a><span class="s2">df = df.merge(poverty, on=[</span><span class="s3">'fips'</span><span class="s2">], how=</span><span class="s3">'left'</span><span class="s2">)</span>
<a name="l183"><span class="ln">183  </span></a><span class="s2">df.drop(columns=[</span><span class="s3">'county_y'</span><span class="s2">, </span><span class="s3">'state_y'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l184"><span class="ln">184  </span></a><span class="s2">df.rename(columns={</span><span class="s3">'county_x'</span><span class="s2">: </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'state_x'</span><span class="s2">: </span><span class="s3">'state'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l185"><span class="ln">185  </span></a><span class="s2">df.head(</span><span class="s4">5</span><span class="s2">)</span>
<a name="l186"><span class="ln">186  </span></a><span class="s0">#%% md 
<a name="l187"><span class="ln">187  </span></a></span><span class="s2">### Data Cleaning 
<a name="l188"><span class="ln">188  </span></a></span><span class="s0">#%% 
<a name="l189"><span class="ln">189  </span></a></span><span class="s2">df.drop_duplicates(subset=[</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'unemployment_rate'</span><span class="s2">, </span><span class="s3">'poverty_allages'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l190"><span class="ln">190  </span></a><span class="s2">df[</span><span class="s3">'poverty_allages'</span><span class="s2">] = df[</span><span class="s3">'poverty_allages'</span><span class="s2">].astype(float)</span>
<a name="l191"><span class="ln">191  </span></a><span class="s2">df[</span><span class="s3">'some_college'</span><span class="s2">] = df[</span><span class="s3">'some_college'</span><span class="s2">].astype(float)</span>
<a name="l192"><span class="ln">192  </span></a><span class="s2">df[</span><span class="s3">'bachelor_or_higher'</span><span class="s2">] = df[</span><span class="s3">'bachelor_or_higher'</span><span class="s2">].astype(float)</span>
<a name="l193"><span class="ln">193  </span></a><span class="s2">df[</span><span class="s3">'high_school'</span><span class="s2">] = df[</span><span class="s3">'high_school'</span><span class="s2">].astype(float)</span>
<a name="l194"><span class="ln">194  </span></a><span class="s2">df[</span><span class="s3">'less_than_high_school'</span><span class="s2">] = df[</span><span class="s3">'less_than_high_school'</span><span class="s2">].astype(float)</span>
<a name="l195"><span class="ln">195  </span></a><span class="s2">df[</span><span class="s3">'unemployment_rate'</span><span class="s2">] = df[</span><span class="s3">'unemployment_rate'</span><span class="s2">].astype(float)</span>
<a name="l196"><span class="ln">196  </span></a><span class="s2">df[</span><span class="s3">'unemployed'</span><span class="s2">] = df[</span><span class="s3">'unemployed'</span><span class="s2">].astype(float)</span>
<a name="l197"><span class="ln">197  </span></a><span class="s2">df.rename(columns={</span><span class="s3">'poverty_allages'</span><span class="s2">: </span><span class="s3">'poverty_rate'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l198"><span class="ln">198  </span></a><span class="s2">df.drop(columns=[</span><span class="s3">'year'</span><span class="s2">, </span><span class="s3">'county, state'</span><span class="s2">, </span><span class="s3">'state_fips'</span><span class="s2">, </span><span class="s3">'county_fips'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l199"><span class="ln">199  </span></a><span class="s2">df[</span><span class="s3">'daily_cases_100k'</span><span class="s2">] = df[</span><span class="s3">'daily_cases'</span><span class="s2">] / df[</span><span class="s3">'population'</span><span class="s2">] * </span><span class="s4">100000</span>
<a name="l200"><span class="ln">200  </span></a><span class="s2">df[</span><span class="s3">'daily_deaths_100k'</span><span class="s2">] = df[</span><span class="s3">'daily_deaths'</span><span class="s2">] / df[</span><span class="s3">'population'</span><span class="s2">] * </span><span class="s4">100000</span>
<a name="l201"><span class="ln">201  </span></a><span class="s2">df.head(</span><span class="s4">5</span><span class="s2">)</span>
<a name="l202"><span class="ln">202  </span></a><span class="s0">#%% md 
<a name="l203"><span class="ln">203  </span></a></span><span class="s2">### Check Missing Values 
<a name="l204"><span class="ln">204  </span></a></span><span class="s0">#%% 
<a name="l205"><span class="ln">205  </span></a></span><span class="s2">df.reset_index(drop=</span><span class="s1">True</span><span class="s2">, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l206"><span class="ln">206  </span></a><span class="s0">#%% 
<a name="l207"><span class="ln">207  </span></a></span><span class="s2">result = df.isnull().sum() / len(df)</span>
<a name="l208"><span class="ln">208  </span></a><span class="s2">result</span>
<a name="l209"><span class="ln">209  </span></a><span class="s0">#%% 
<a name="l210"><span class="ln">210  </span></a></span><span class="s2">data = df</span>
<a name="l211"><span class="ln">211  </span></a>
<a name="l212"><span class="ln">212  </span></a><span class="s0"># Create a dictionary to store missing dates for each FIPS</span>
<a name="l213"><span class="ln">213  </span></a><span class="s2">missing_dates = {}</span>
<a name="l214"><span class="ln">214  </span></a>
<a name="l215"><span class="ln">215  </span></a><span class="s0"># Get the unique FIPS values</span>
<a name="l216"><span class="ln">216  </span></a><span class="s2">unique_fips = data[</span><span class="s3">'fips'</span><span class="s2">].unique()</span>
<a name="l217"><span class="ln">217  </span></a>
<a name="l218"><span class="ln">218  </span></a><span class="s2">expected_dates = pd.date_range(start=</span><span class="s3">'2020-03-15'</span><span class="s2">, end=</span><span class="s3">'2021-10-15'</span><span class="s2">)</span>
<a name="l219"><span class="ln">219  </span></a>
<a name="l220"><span class="ln">220  </span></a><span class="s0"># Check for missing dates for each FIPS</span>
<a name="l221"><span class="ln">221  </span></a><span class="s1">for </span><span class="s2">fips </span><span class="s1">in </span><span class="s2">unique_fips:</span>
<a name="l222"><span class="ln">222  </span></a>    <span class="s2">fips_data = data[data[</span><span class="s3">'fips'</span><span class="s2">] == fips]</span>
<a name="l223"><span class="ln">223  </span></a>    <span class="s2">fips_dates = pd.to_datetime(fips_data[</span><span class="s3">'date'</span><span class="s2">])</span>
<a name="l224"><span class="ln">224  </span></a>    <span class="s2">missing_fips_dates = expected_dates.difference(fips_dates)</span>
<a name="l225"><span class="ln">225  </span></a>    <span class="s2">missing_dates[fips] = missing_fips_dates.tolist()</span>
<a name="l226"><span class="ln">226  </span></a>
<a name="l227"><span class="ln">227  </span></a><span class="s0">#%% 
<a name="l228"><span class="ln">228  </span></a></span><span class="s2">missing_fips = []</span>
<a name="l229"><span class="ln">229  </span></a><span class="s2">values_fips = []</span>
<a name="l230"><span class="ln">230  </span></a>
<a name="l231"><span class="ln">231  </span></a><span class="s1">for </span><span class="s2">fips, dates </span><span class="s1">in </span><span class="s2">missing_dates.items():</span>
<a name="l232"><span class="ln">232  </span></a>    <span class="s1">if </span><span class="s2">len(dates) &gt; </span><span class="s4">30</span><span class="s2">:</span>
<a name="l233"><span class="ln">233  </span></a>        <span class="s2">missing_fips.append(fips)</span>
<a name="l234"><span class="ln">234  </span></a>        <span class="s2">values_fips.append(len(dates) / len(expected_dates) * </span><span class="s4">100</span><span class="s2">)</span>
<a name="l235"><span class="ln">235  </span></a><span class="s0">#%% 
<a name="l236"><span class="ln">236  </span></a></span><span class="s2">len(missing_fips) / len(unique_fips)</span>
<a name="l237"><span class="ln">237  </span></a><span class="s0">#%% 
<a name="l238"><span class="ln">238  </span></a></span><span class="s2">missing = pd.DataFrame(list(zip(missing_fips, values_fips)),</span>
<a name="l239"><span class="ln">239  </span></a>                       <span class="s2">columns=[</span><span class="s3">'FIPS'</span><span class="s2">, </span><span class="s3">'Missing Dates'</span><span class="s2">])</span>
<a name="l240"><span class="ln">240  </span></a><span class="s2">missing</span>
<a name="l241"><span class="ln">241  </span></a><span class="s0">#%% 
<a name="l242"><span class="ln">242  </span></a></span><span class="s1">import </span><span class="s2">plotly.express </span><span class="s1">as </span><span class="s2">px</span>
<a name="l243"><span class="ln">243  </span></a>
<a name="l244"><span class="ln">244  </span></a><span class="s1">from </span><span class="s2">urllib.request </span><span class="s1">import </span><span class="s2">urlopen</span>
<a name="l245"><span class="ln">245  </span></a><span class="s1">import </span><span class="s2">json</span>
<a name="l246"><span class="ln">246  </span></a>
<a name="l247"><span class="ln">247  </span></a><span class="s1">with </span><span class="s2">urlopen(</span><span class="s3">'https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json'</span><span class="s2">) </span><span class="s1">as </span><span class="s2">response:</span>
<a name="l248"><span class="ln">248  </span></a>    <span class="s2">counties = json.load(response)</span>
<a name="l249"><span class="ln">249  </span></a>
<a name="l250"><span class="ln">250  </span></a><span class="s2">fig = px.choropleth(missing, geojson=counties, locations=</span><span class="s3">'FIPS'</span><span class="s2">,</span>
<a name="l251"><span class="ln">251  </span></a>                    <span class="s2">color=</span><span class="s3">'Missing Dates'</span><span class="s2">,</span>
<a name="l252"><span class="ln">252  </span></a>                    <span class="s2">color_continuous_scale=</span><span class="s3">&quot;Viridis&quot;</span><span class="s2">,</span>
<a name="l253"><span class="ln">253  </span></a>                    <span class="s2">color_continuous_midpoint=</span><span class="s4">2</span><span class="s2">,</span>
<a name="l254"><span class="ln">254  </span></a>                    <span class="s2">range_color=(</span><span class="s4">10</span><span class="s2">, </span><span class="s4">80</span><span class="s2">),</span>
<a name="l255"><span class="ln">255  </span></a>                    <span class="s2">scope=</span><span class="s3">&quot;usa&quot;</span><span class="s2">,</span>
<a name="l256"><span class="ln">256  </span></a>                    <span class="s2">hover_name=</span><span class="s3">'FIPS'</span>
<a name="l257"><span class="ln">257  </span></a>                    <span class="s2">)</span>
<a name="l258"><span class="ln">258  </span></a><span class="s2">fig.update_layout(margin={</span><span class="s3">&quot;r&quot;</span><span class="s2">: </span><span class="s4">0</span><span class="s2">, </span><span class="s3">&quot;t&quot;</span><span class="s2">: </span><span class="s4">0</span><span class="s2">, </span><span class="s3">&quot;l&quot;</span><span class="s2">: </span><span class="s4">0</span><span class="s2">, </span><span class="s3">&quot;b&quot;</span><span class="s2">: </span><span class="s4">0</span><span class="s2">})</span>
<a name="l259"><span class="ln">259  </span></a><span class="s2">fig.show()</span>
<a name="l260"><span class="ln">260  </span></a><span class="s2">fig.write_image(</span><span class="s3">&quot;output/graph/missing.png&quot;</span><span class="s2">)</span>
<a name="l261"><span class="ln">261  </span></a><span class="s0">#%% 
<a name="l262"><span class="ln">262  </span></a></span><span class="s2">df.reset_index(inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l263"><span class="ln">263  </span></a><span class="s2">df[</span><span class="s3">'state'</span><span class="s2">]</span>
<a name="l264"><span class="ln">264  </span></a><span class="s0">#%% 
<a name="l265"><span class="ln">265  </span></a></span><span class="s2">state_fips = df.groupby(by=[</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'state'</span><span class="s2">], as_index=</span><span class="s1">False</span><span class="s2">).count().iloc[:, </span><span class="s4">0</span><span class="s2">:</span><span class="s4">2</span><span class="s2">]</span>
<a name="l266"><span class="ln">266  </span></a><span class="s2">missing.rename(columns={</span><span class="s3">'FIPS'</span><span class="s2">: </span><span class="s3">'fips'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l267"><span class="ln">267  </span></a><span class="s2">state_fips = state_fips.merge(missing, on=[</span><span class="s3">'fips'</span><span class="s2">], how=</span><span class="s3">'right'</span><span class="s2">)</span>
<a name="l268"><span class="ln">268  </span></a><span class="s2">state_fips</span>
<a name="l269"><span class="ln">269  </span></a><span class="s0">#%% 
<a name="l270"><span class="ln">270  </span></a></span><span class="s2">least_missing = state_fips.groupby([</span><span class="s3">'state'</span><span class="s2">], as_index=</span><span class="s1">False</span><span class="s2">).mean().sort_values(by=[</span><span class="s3">'Missing Dates'</span><span class="s2">],</span>
<a name="l271"><span class="ln">271  </span></a>                                                                                 <span class="s2">ascending=</span><span class="s1">True</span><span class="s2">).reset_index(drop=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l272"><span class="ln">272  </span></a><span class="s2">least_missing.rename(columns={</span><span class="s3">'Missing Dates'</span><span class="s2">: </span><span class="s3">'Percent of Missing Dates'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l273"><span class="ln">273  </span></a><span class="s2">least_missing = least_missing[</span><span class="s3">'state'</span><span class="s2">].tolist()</span>
<a name="l274"><span class="ln">274  </span></a><span class="s2">print(least_missing)</span>
<a name="l275"><span class="ln">275  </span></a><span class="s0">#%% 
<a name="l276"><span class="ln">276  </span></a></span><span class="s2">least_missing_count = state_fips.groupby([</span><span class="s3">'state'</span><span class="s2">], as_index=</span><span class="s1">False</span><span class="s2">).count().sort_values(by=[</span><span class="s3">'Missing Dates'</span><span class="s2">],</span>
<a name="l277"><span class="ln">277  </span></a>                                                                                        <span class="s2">ascending=</span><span class="s1">True</span><span class="s2">).reset_index(</span>
<a name="l278"><span class="ln">278  </span></a>    <span class="s2">drop=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l279"><span class="ln">279  </span></a><span class="s2">least_missing_count.rename(columns={</span><span class="s3">'Missing Dates'</span><span class="s2">: </span><span class="s3">'Number of Counties'</span><span class="s2">}, inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l280"><span class="ln">280  </span></a><span class="s2">least_missing_count.drop(columns=[</span><span class="s3">'fips'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l281"><span class="ln">281  </span></a><span class="s2">number_counties_per_state = df.groupby([</span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'fips'</span><span class="s2">], as_index=</span><span class="s1">False</span><span class="s2">).count()[[</span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'fips'</span><span class="s2">]].groupby([</span><span class="s3">'state'</span><span class="s2">],</span>
<a name="l282"><span class="ln">282  </span></a>                                                                                                             <span class="s2">as_index=</span><span class="s1">False</span><span class="s2">).count().sort_values(</span>
<a name="l283"><span class="ln">283  </span></a>    <span class="s2">by=[</span><span class="s3">'fips'</span><span class="s2">], ascending=</span><span class="s1">True</span><span class="s2">).reset_index(drop=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l284"><span class="ln">284  </span></a><span class="s2">least_missing_count = least_missing_count.merge(number_counties_per_state, on=[</span><span class="s3">'state'</span><span class="s2">], how=</span><span class="s3">'left'</span><span class="s2">)</span>
<a name="l285"><span class="ln">285  </span></a><span class="s2">least_missing_count[</span><span class="s3">'Percent of Counties'</span><span class="s2">] = least_missing_count[</span><span class="s3">'Number of Counties'</span><span class="s2">] / least_missing_count[</span><span class="s3">'fips'</span><span class="s2">]</span>
<a name="l286"><span class="ln">286  </span></a><span class="s2">least_missing_count = \</span>
<a name="l287"><span class="ln">287  </span></a>    <span class="s2">least_missing_count.sort_values(by=[</span><span class="s3">'Percent of Counties'</span><span class="s2">], ascending=</span><span class="s1">True</span><span class="s2">).reset_index(drop=</span><span class="s1">True</span><span class="s2">)[</span><span class="s3">'state'</span><span class="s2">].tolist()</span>
<a name="l288"><span class="ln">288  </span></a><span class="s2">print(least_missing_count)</span>
<a name="l289"><span class="ln">289  </span></a><span class="s0">#%% 
<a name="l290"><span class="ln">290  </span></a></span><span class="s2">common_states = list(set(least_missing[</span><span class="s4">0</span><span class="s2">:</span><span class="s4">10</span><span class="s2">]) &amp; set(least_missing_count[</span><span class="s4">0</span><span class="s2">:</span><span class="s4">10</span><span class="s2">]))</span>
<a name="l291"><span class="ln">291  </span></a><span class="s2">print(common_states)</span>
<a name="l292"><span class="ln">292  </span></a><span class="s0">#%% md 
<a name="l293"><span class="ln">293  </span></a></span><span class="s2">As a result, this project will focus on the following states: 
<a name="l294"><span class="ln">294  </span></a>- New York 
<a name="l295"><span class="ln">295  </span></a>- Maryland 
<a name="l296"><span class="ln">296  </span></a>- Pennsylvania 
<a name="l297"><span class="ln">297  </span></a> 
<a name="l298"><span class="ln">298  </span></a>And use the following states as supplementary: 
<a name="l299"><span class="ln">299  </span></a>- California 
<a name="l300"><span class="ln">300  </span></a>- Maine 
<a name="l301"><span class="ln">301  </span></a>- Ohio 
<a name="l302"><span class="ln">302  </span></a>- South Carolina 
<a name="l303"><span class="ln">303  </span></a></span><span class="s0">#%% 
<a name="l304"><span class="ln">304  </span></a></span><span class="s2">df = pd.get_dummies(df, columns=[</span><span class="s3">'party'</span><span class="s2">])</span>
<a name="l305"><span class="ln">305  </span></a><span class="s0">#%% 
<a name="l306"><span class="ln">306  </span></a></span><span class="s2">ca = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'California'</span><span class="s2">]</span>
<a name="l307"><span class="ln">307  </span></a><span class="s2">me = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'Maine'</span><span class="s2">]</span>
<a name="l308"><span class="ln">308  </span></a><span class="s2">md = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'Maryland'</span><span class="s2">]</span>
<a name="l309"><span class="ln">309  </span></a><span class="s2">ny = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'New York'</span><span class="s2">]</span>
<a name="l310"><span class="ln">310  </span></a><span class="s2">oh = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'Ohio'</span><span class="s2">]</span>
<a name="l311"><span class="ln">311  </span></a><span class="s2">pa = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'Pennsylvania'</span><span class="s2">]</span>
<a name="l312"><span class="ln">312  </span></a><span class="s2">sa = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'South Carolina'</span><span class="s2">]</span>
<a name="l313"><span class="ln">313  </span></a><span class="s2">population_density = df.groupby([</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'population_density'</span><span class="s2">]).count().reset_index()</span>
<a name="l314"><span class="ln">314  </span></a><span class="s2">ca.to_csv(</span><span class="s3">'output/data/ca.csv'</span><span class="s2">, index=</span><span class="s1">False</span><span class="s2">)</span>
<a name="l315"><span class="ln">315  </span></a><span class="s2">me.to_csv(</span><span class="s3">'output/data/me.csv'</span><span class="s2">, index=</span><span class="s1">False</span><span class="s2">)</span>
<a name="l316"><span class="ln">316  </span></a><span class="s2">md.to_csv(</span><span class="s3">'output/data/md.csv'</span><span class="s2">, index=</span><span class="s1">False</span><span class="s2">)</span>
<a name="l317"><span class="ln">317  </span></a><span class="s2">ny.to_csv(</span><span class="s3">'output/data/ny.csv'</span><span class="s2">, index=</span><span class="s1">False</span><span class="s2">)</span>
<a name="l318"><span class="ln">318  </span></a><span class="s2">oh.to_csv(</span><span class="s3">'output/data/oh.csv'</span><span class="s2">, index=</span><span class="s1">False</span><span class="s2">)</span>
<a name="l319"><span class="ln">319  </span></a><span class="s2">pa.to_csv(</span><span class="s3">'output/data/pa.csv'</span><span class="s2">, index=</span><span class="s1">False</span><span class="s2">)</span>
<a name="l320"><span class="ln">320  </span></a><span class="s2">sa.to_csv(</span><span class="s3">'output/data/sa.csv'</span><span class="s2">, index=</span><span class="s1">False</span><span class="s2">)</span>
<a name="l321"><span class="ln">321  </span></a><span class="s0">#%% md 
<a name="l322"><span class="ln">322  </span></a></span>
<a name="l323"><span class="ln">323  </span></a><span class="s2">## Exploratory Data Analysis 
<a name="l324"><span class="ln">324  </span></a>### Visualize the Data 
<a name="l325"><span class="ln">325  </span></a></span><span class="s0">#%% 
<a name="l326"><span class="ln">326  </span></a></span><span class="s1">def </span><span class="s2">eda(df):</span>
<a name="l327"><span class="ln">327  </span></a>    <span class="s2">curr_state = df[</span><span class="s3">'state'</span><span class="s2">].unique()[</span><span class="s4">0</span><span class="s2">]</span>
<a name="l328"><span class="ln">328  </span></a>    <span class="s2">top_pop_dens = population_density[population_density[</span><span class="s3">'state'</span><span class="s2">] == curr_state].sort_values(by=[</span><span class="s3">'population_density'</span><span class="s2">],</span>
<a name="l329"><span class="ln">329  </span></a>                                                                                             <span class="s2">ascending=</span><span class="s1">False</span><span class="s2">).reset_index(</span>
<a name="l330"><span class="ln">330  </span></a>        <span class="s2">drop=</span><span class="s1">True</span><span class="s2">)[</span><span class="s3">'fips'</span><span class="s2">].tolist()[</span><span class="s4">0</span><span class="s2">:</span><span class="s4">10</span><span class="s2">]</span>
<a name="l331"><span class="ln">331  </span></a>    <span class="s2">df = df[df[</span><span class="s3">'fips'</span><span class="s2">].isin(top_pop_dens)]</span>
<a name="l332"><span class="ln">332  </span></a>
<a name="l333"><span class="ln">333  </span></a>    <span class="s2">fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(</span><span class="s4">2</span><span class="s2">, </span><span class="s4">3</span><span class="s2">, figsize=(</span><span class="s4">60</span><span class="s2">, </span><span class="s4">30</span><span class="s2">))</span>
<a name="l334"><span class="ln">334  </span></a>
<a name="l335"><span class="ln">335  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'retail_and_recreation'</span><span class="s2">,</span>
<a name="l336"><span class="ln">336  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax1)</span>
<a name="l337"><span class="ln">337  </span></a>    <span class="s2">ax1.set_title(</span><span class="s3">'Retail and Recreation Mobility Change in %s' </span><span class="s2">% curr_state)</span>
<a name="l338"><span class="ln">338  </span></a>    <span class="s2">ax1.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l339"><span class="ln">339  </span></a>    <span class="s2">ax1.set_ylabel(</span><span class="s3">'Mobility Change (%), Retail and Recreation'</span><span class="s2">)</span>
<a name="l340"><span class="ln">340  </span></a>
<a name="l341"><span class="ln">341  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'grocery_and_pharmacy'</span><span class="s2">,</span>
<a name="l342"><span class="ln">342  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax2)</span>
<a name="l343"><span class="ln">343  </span></a>    <span class="s2">ax2.set_title(</span><span class="s3">'Grocery and Pharmacy Mobility Change in %s' </span><span class="s2">% curr_state)</span>
<a name="l344"><span class="ln">344  </span></a>    <span class="s2">ax2.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l345"><span class="ln">345  </span></a>    <span class="s2">ax2.set_ylabel(</span><span class="s3">'Mobility Change (%), Grocery and Pharmacy'</span><span class="s2">)</span>
<a name="l346"><span class="ln">346  </span></a>
<a name="l347"><span class="ln">347  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'parks'</span><span class="s2">,</span>
<a name="l348"><span class="ln">348  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax3)</span>
<a name="l349"><span class="ln">349  </span></a>    <span class="s2">ax3.set_title(</span><span class="s3">'Parks Mobility Change in %s' </span><span class="s2">% curr_state)</span>
<a name="l350"><span class="ln">350  </span></a>    <span class="s2">ax3.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l351"><span class="ln">351  </span></a>    <span class="s2">ax3.set_ylabel(</span><span class="s3">'Mobility Change (%), Parks'</span><span class="s2">)</span>
<a name="l352"><span class="ln">352  </span></a>
<a name="l353"><span class="ln">353  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'transit_stations'</span><span class="s2">,</span>
<a name="l354"><span class="ln">354  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax4)</span>
<a name="l355"><span class="ln">355  </span></a>    <span class="s2">ax4.set_title(</span><span class="s3">'Transit Stations Mobility Change in %s' </span><span class="s2">% curr_state)</span>
<a name="l356"><span class="ln">356  </span></a>    <span class="s2">ax4.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l357"><span class="ln">357  </span></a>    <span class="s2">ax4.set_ylabel(</span><span class="s3">'Mobility Change (%), Transit Stations'</span><span class="s2">)</span>
<a name="l358"><span class="ln">358  </span></a>
<a name="l359"><span class="ln">359  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'workplaces'</span><span class="s2">,</span>
<a name="l360"><span class="ln">360  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax5)</span>
<a name="l361"><span class="ln">361  </span></a>    <span class="s2">ax5.set_title(</span><span class="s3">'Workplaces Mobility Change in %s' </span><span class="s2">% curr_state)</span>
<a name="l362"><span class="ln">362  </span></a>    <span class="s2">ax5.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l363"><span class="ln">363  </span></a>    <span class="s2">ax5.set_ylabel(</span><span class="s3">'Mobility Change (%), Workplaces'</span><span class="s2">)</span>
<a name="l364"><span class="ln">364  </span></a>
<a name="l365"><span class="ln">365  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'residential'</span><span class="s2">,</span>
<a name="l366"><span class="ln">366  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax6)</span>
<a name="l367"><span class="ln">367  </span></a>    <span class="s2">ax6.set_title(</span><span class="s3">'Residential Mobility Change in %s' </span><span class="s2">% curr_state)</span>
<a name="l368"><span class="ln">368  </span></a>    <span class="s2">ax6.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l369"><span class="ln">369  </span></a>    <span class="s2">ax6.set_ylabel(</span><span class="s3">'Mobility Change (%), Residential'</span><span class="s2">)</span>
<a name="l370"><span class="ln">370  </span></a>
<a name="l371"><span class="ln">371  </span></a>    <span class="s2">plt.savefig(</span><span class="s3">'output/graph/mobility_%s.png' </span><span class="s2">% curr_state)</span>
<a name="l372"><span class="ln">372  </span></a>
<a name="l373"><span class="ln">373  </span></a>    <span class="s2">fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(</span><span class="s4">2</span><span class="s2">, </span><span class="s4">2</span><span class="s2">, figsize=(</span><span class="s4">20</span><span class="s2">, </span><span class="s4">20</span><span class="s2">))</span>
<a name="l374"><span class="ln">374  </span></a>
<a name="l375"><span class="ln">375  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'daily_cases_100k'</span><span class="s2">,</span>
<a name="l376"><span class="ln">376  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax1)</span>
<a name="l377"><span class="ln">377  </span></a>    <span class="s2">ax1.set_title(</span><span class="s3">'Cases per 100k in %s' </span><span class="s2">% curr_state)</span>
<a name="l378"><span class="ln">378  </span></a>    <span class="s2">ax1.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l379"><span class="ln">379  </span></a>    <span class="s2">ax1.set_ylabel(</span><span class="s3">'Cases per 100k'</span><span class="s2">)</span>
<a name="l380"><span class="ln">380  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'daily_deaths_100k'</span><span class="s2">,</span>
<a name="l381"><span class="ln">381  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax2)</span>
<a name="l382"><span class="ln">382  </span></a>    <span class="s2">ax2.set_title(</span><span class="s3">'Deaths per 100k in %s' </span><span class="s2">% curr_state)</span>
<a name="l383"><span class="ln">383  </span></a>    <span class="s2">ax2.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l384"><span class="ln">384  </span></a>    <span class="s2">ax2.set_ylabel(</span><span class="s3">'Deaths per 100k'</span><span class="s2">)</span>
<a name="l385"><span class="ln">385  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'daily_cases'</span><span class="s2">,</span>
<a name="l386"><span class="ln">386  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax3)</span>
<a name="l387"><span class="ln">387  </span></a>    <span class="s2">ax3.set_title(</span><span class="s3">'Daily New Cases in %s' </span><span class="s2">% curr_state)</span>
<a name="l388"><span class="ln">388  </span></a>    <span class="s2">ax3.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l389"><span class="ln">389  </span></a>    <span class="s2">ax3.set_ylabel(</span><span class="s3">'New Cases'</span><span class="s2">)</span>
<a name="l390"><span class="ln">390  </span></a>    <span class="s2">sns.lineplot(data=df, x=</span><span class="s3">'date'</span><span class="s2">, y=</span><span class="s3">'daily_deaths'</span><span class="s2">,</span>
<a name="l391"><span class="ln">391  </span></a>                 <span class="s2">hue=</span><span class="s3">'county'</span><span class="s2">, palette=</span><span class="s3">'tab10'</span><span class="s2">, ax=ax4)</span>
<a name="l392"><span class="ln">392  </span></a>    <span class="s2">ax4.set_title(</span><span class="s3">'Daily New Deaths in %s' </span><span class="s2">% curr_state)</span>
<a name="l393"><span class="ln">393  </span></a>    <span class="s2">ax4.set_xlabel(</span><span class="s3">'Date'</span><span class="s2">)</span>
<a name="l394"><span class="ln">394  </span></a>    <span class="s2">ax4.set_ylabel(</span><span class="s3">'New Deaths'</span><span class="s2">)</span>
<a name="l395"><span class="ln">395  </span></a>
<a name="l396"><span class="ln">396  </span></a>    <span class="s2">plt.savefig(</span><span class="s3">'output/graph/cases_%s.png' </span><span class="s2">% curr_state)</span>
<a name="l397"><span class="ln">397  </span></a><span class="s0">#%% 
<a name="l398"><span class="ln">398  </span></a></span><span class="s2">states = [ca, me, md, ny, oh, pa, sa]</span>
<a name="l399"><span class="ln">399  </span></a><span class="s0">#for i in states:</span>
<a name="l400"><span class="ln">400  </span></a><span class="s0">#   eda(i)</span>
<a name="l401"><span class="ln">401  </span></a><span class="s0">#%% 
<a name="l402"><span class="ln">402  </span></a></span><span class="s2">ca = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'California'</span><span class="s2">]</span>
<a name="l403"><span class="ln">403  </span></a><span class="s2">me = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'Maine'</span><span class="s2">]</span>
<a name="l404"><span class="ln">404  </span></a><span class="s2">md = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'Maryland'</span><span class="s2">]</span>
<a name="l405"><span class="ln">405  </span></a><span class="s2">ny = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'New York'</span><span class="s2">]</span>
<a name="l406"><span class="ln">406  </span></a><span class="s2">oh = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'Ohio'</span><span class="s2">]</span>
<a name="l407"><span class="ln">407  </span></a><span class="s2">pa = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'Pennsylvania'</span><span class="s2">]</span>
<a name="l408"><span class="ln">408  </span></a><span class="s2">sa = df[df[</span><span class="s3">'state'</span><span class="s2">] == </span><span class="s3">'South Carolina'</span><span class="s2">]</span>
<a name="l409"><span class="ln">409  </span></a><span class="s2">population_density = df.groupby([</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'population_density'</span><span class="s2">]).count().reset_index()</span>
<a name="l410"><span class="ln">410  </span></a><span class="s2">states = [ca, me, md, ny, oh, pa, sa]</span>
<a name="l411"><span class="ln">411  </span></a><span class="s0">#%% md 
<a name="l412"><span class="ln">412  </span></a></span><span class="s2">#### Correlation Matrix 
<a name="l413"><span class="ln">413  </span></a></span><span class="s0">#%% 
<a name="l414"><span class="ln">414  </span></a></span><span class="s1">def </span><span class="s2">corr_matrix(df):</span>
<a name="l415"><span class="ln">415  </span></a>    <span class="s2">curr_state = df[</span><span class="s3">'state'</span><span class="s2">].unique()[</span><span class="s4">0</span><span class="s2">]</span>
<a name="l416"><span class="ln">416  </span></a>    <span class="s2">top_pop_dens = population_density[population_density[</span><span class="s3">'state'</span><span class="s2">] == curr_state].sort_values(by=[</span><span class="s3">'population_density'</span><span class="s2">],</span>
<a name="l417"><span class="ln">417  </span></a>                                                                                             <span class="s2">ascending=</span><span class="s1">False</span><span class="s2">).reset_index(</span>
<a name="l418"><span class="ln">418  </span></a>        <span class="s2">drop=</span><span class="s1">True</span><span class="s2">)[</span><span class="s3">'fips'</span><span class="s2">].tolist()[</span><span class="s4">0</span><span class="s2">:</span><span class="s4">10</span><span class="s2">]</span>
<a name="l419"><span class="ln">419  </span></a>    <span class="s2">df = df[df[</span><span class="s3">'fips'</span><span class="s2">].isin(top_pop_dens)]</span>
<a name="l420"><span class="ln">420  </span></a>    <span class="s2">df = df.drop([</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'county'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">], axis=</span><span class="s4">1</span><span class="s2">)</span>
<a name="l421"><span class="ln">421  </span></a>    <span class="s2">corr = df.corr()</span>
<a name="l422"><span class="ln">422  </span></a>    <span class="s2">fig, ax = plt.subplots(figsize=(</span><span class="s4">20</span><span class="s2">, </span><span class="s4">20</span><span class="s2">))</span>
<a name="l423"><span class="ln">423  </span></a>    <span class="s2">sns.heatmap(corr, annot=</span><span class="s1">True</span><span class="s2">, fmt=</span><span class="s3">'.2f'</span><span class="s2">, cmap=</span><span class="s3">'coolwarm'</span><span class="s2">, ax=ax)</span>
<a name="l424"><span class="ln">424  </span></a>    <span class="s2">plt.savefig(</span><span class="s3">'output/graph/corr_%s.png' </span><span class="s2">% curr_state)</span>
<a name="l425"><span class="ln">425  </span></a><span class="s0">#%% md 
<a name="l426"><span class="ln">426  </span></a></span><span class="s2">### Feature Engineering 
<a name="l427"><span class="ln">427  </span></a></span><span class="s0">#%% 
<a name="l428"><span class="ln">428  </span></a></span><span class="s1">def </span><span class="s2">feature_engineering(df):</span>
<a name="l429"><span class="ln">429  </span></a>    <span class="s2">df[</span><span class="s3">'date'</span><span class="s2">] = pd.to_datetime(df[</span><span class="s3">'date'</span><span class="s2">])</span>
<a name="l430"><span class="ln">430  </span></a>    <span class="s2">df[</span><span class="s3">'month'</span><span class="s2">] = df[</span><span class="s3">'date'</span><span class="s2">].dt.month</span>
<a name="l431"><span class="ln">431  </span></a>    <span class="s2">df[</span><span class="s3">'day'</span><span class="s2">] = df[</span><span class="s3">'date'</span><span class="s2">].dt.day</span>
<a name="l432"><span class="ln">432  </span></a>    <span class="s2">df[</span><span class="s3">'day_of_week'</span><span class="s2">] = df[</span><span class="s3">'date'</span><span class="s2">].dt.dayofweek</span>
<a name="l433"><span class="ln">433  </span></a>    <span class="s2">df[</span><span class="s3">'weekend'</span><span class="s2">] = df[</span><span class="s3">'day_of_week'</span><span class="s2">].apply(</span><span class="s1">lambda </span><span class="s2">x: </span><span class="s4">1 </span><span class="s1">if </span><span class="s2">x &gt;= </span><span class="s4">5 </span><span class="s1">else </span><span class="s4">0</span><span class="s2">)</span>
<a name="l434"><span class="ln">434  </span></a>    <span class="s2">df = df.drop([</span><span class="s3">'fips'</span><span class="s2">, </span><span class="s3">'state'</span><span class="s2">, </span><span class="s3">'county'</span><span class="s2">], axis=</span><span class="s4">1</span><span class="s2">)</span>
<a name="l435"><span class="ln">435  </span></a>    <span class="s2">df[</span><span class="s3">'duration'</span><span class="s2">] = (df[</span><span class="s3">'date'</span><span class="s2">] - df[</span><span class="s3">'date'</span><span class="s2">].min()).dt.days</span>
<a name="l436"><span class="ln">436  </span></a>    <span class="s0">#df['log_cases'] = np.log(df['cases'] + 1)</span>
<a name="l437"><span class="ln">437  </span></a>    <span class="s0">#df['log_deaths'] = np.log(df['deaths'] + 1)</span>
<a name="l438"><span class="ln">438  </span></a>    <span class="s2">df = pd.get_dummies(df, columns=[</span><span class="s3">'month'</span><span class="s2">, </span><span class="s3">'day_of_week'</span><span class="s2">])</span>
<a name="l439"><span class="ln">439  </span></a>    <span class="s2">df = df.drop([</span><span class="s3">'month_1'</span><span class="s2">, </span><span class="s3">'day_of_week_0'</span><span class="s2">], axis=</span><span class="s4">1</span><span class="s2">)</span>
<a name="l440"><span class="ln">440  </span></a>    <span class="s0"># Define the features to transform</span>
<a name="l441"><span class="ln">441  </span></a>    <span class="s2">features = [</span><span class="s3">'cases'</span><span class="s2">, </span><span class="s3">'deaths'</span><span class="s2">, </span><span class="s3">'daily_cases'</span><span class="s2">, </span><span class="s3">'daily_deaths'</span><span class="s2">,</span>
<a name="l442"><span class="ln">442  </span></a>                <span class="s3">'retail_and_recreation'</span><span class="s2">, </span><span class="s3">'grocery_and_pharmacy'</span><span class="s2">, </span><span class="s3">'parks'</span><span class="s2">,</span>
<a name="l443"><span class="ln">443  </span></a>                <span class="s3">'transit_stations'</span><span class="s2">, </span><span class="s3">'workplaces'</span><span class="s2">, </span><span class="s3">'residential'</span><span class="s2">, </span><span class="s3">'population'</span><span class="s2">,</span>
<a name="l444"><span class="ln">444  </span></a>                <span class="s3">'land_area'</span><span class="s2">, </span><span class="s3">'population_density'</span><span class="s2">, </span><span class="s3">'labor_force'</span><span class="s2">, </span><span class="s3">'employed'</span><span class="s2">,</span>
<a name="l445"><span class="ln">445  </span></a>                <span class="s3">'unemployed'</span><span class="s2">, </span><span class="s3">'unemployment_rate'</span><span class="s2">, </span><span class="s3">'some_college'</span><span class="s2">, </span><span class="s3">'bachelor_or_higher'</span><span class="s2">,</span>
<a name="l446"><span class="ln">446  </span></a>                <span class="s3">'high_school'</span><span class="s2">, </span><span class="s3">'less_than_high_school'</span><span class="s2">, </span><span class="s3">'poverty_rate'</span><span class="s2">,</span>
<a name="l447"><span class="ln">447  </span></a>                <span class="s3">'daily_cases_100k'</span><span class="s2">, </span><span class="s3">'daily_deaths_100k'</span><span class="s2">]</span>
<a name="l448"><span class="ln">448  </span></a>
<a name="l449"><span class="ln">449  </span></a>    <span class="s0"># Perform Box-Cox transformation on each feature</span>
<a name="l450"><span class="ln">450  </span></a>    <span class="s1">for </span><span class="s2">feature </span><span class="s1">in </span><span class="s2">features:</span>
<a name="l451"><span class="ln">451  </span></a>        <span class="s0"># Transform the feature</span>
<a name="l452"><span class="ln">452  </span></a>        <span class="s0"># Add a small constant to avoid issues with zero values</span>
<a name="l453"><span class="ln">453  </span></a>        <span class="s2">df[feature] += </span><span class="s4">1e-10</span>
<a name="l454"><span class="ln">454  </span></a>        <span class="s1">if </span><span class="s2">df[feature].min() &lt;= </span><span class="s4">0</span><span class="s2">:</span>
<a name="l455"><span class="ln">455  </span></a>            <span class="s1">continue</span>
<a name="l456"><span class="ln">456  </span></a>        <span class="s2">transformed_feature, lambda_param = stats.boxcox(df[feature])</span>
<a name="l457"><span class="ln">457  </span></a>        <span class="s2">df[</span><span class="s3">f'</span><span class="s5">{</span><span class="s2">feature</span><span class="s5">}</span><span class="s3">_transformed'</span><span class="s2">] = transformed_feature</span>
<a name="l458"><span class="ln">458  </span></a>    <span class="s2">df.drop(columns=[</span><span class="s3">'index'</span><span class="s2">, </span><span class="s3">'date'</span><span class="s2">], inplace=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l459"><span class="ln">459  </span></a>
<a name="l460"><span class="ln">460  </span></a>    <span class="s1">return </span><span class="s2">df</span>
<a name="l461"><span class="ln">461  </span></a><span class="s0">#%% 
<a name="l462"><span class="ln">462  </span></a></span><span class="s2">ca = feature_engineering(ca)</span>
<a name="l463"><span class="ln">463  </span></a><span class="s2">me = feature_engineering(me)</span>
<a name="l464"><span class="ln">464  </span></a><span class="s2">md = feature_engineering(md)</span>
<a name="l465"><span class="ln">465  </span></a><span class="s2">ny = feature_engineering(ny)</span>
<a name="l466"><span class="ln">466  </span></a><span class="s2">oh = feature_engineering(oh)</span>
<a name="l467"><span class="ln">467  </span></a><span class="s2">pa = feature_engineering(pa)</span>
<a name="l468"><span class="ln">468  </span></a><span class="s2">sa = feature_engineering(sa)</span>
<a name="l469"><span class="ln">469  </span></a><span class="s0">#%% 
<a name="l470"><span class="ln">470  </span></a></span><span class="s2">df = feature_engineering(df)</span>
<a name="l471"><span class="ln">471  </span></a><span class="s0">#%% 
<a name="l472"><span class="ln">472  </span></a></span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">states:</span>
<a name="l473"><span class="ln">473  </span></a>    <span class="s2">corr_matrix(i)</span>
<a name="l474"><span class="ln">474  </span></a><span class="s0">#%% 
<a name="l475"><span class="ln">475  </span></a></span><span class="s1">from </span><span class="s2">sklearn.preprocessing </span><span class="s1">import </span><span class="s2">StandardScaler</span>
<a name="l476"><span class="ln">476  </span></a><span class="s1">from </span><span class="s2">sklearn.decomposition </span><span class="s1">import </span><span class="s2">PCA</span>
<a name="l477"><span class="ln">477  </span></a>
<a name="l478"><span class="ln">478  </span></a><span class="s0">#%% 
<a name="l479"><span class="ln">479  </span></a></span><span class="s2">X = ca.fillna(</span><span class="s4">0</span><span class="s2">).values  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l480"><span class="ln">480  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l481"><span class="ln">481  </span></a><span class="s2">X_std = sc.fit_transform(X)  </span><span class="s0"># standardizing the data</span>
<a name="l482"><span class="ln">482  </span></a><span class="s2">pca = PCA()</span>
<a name="l483"><span class="ln">483  </span></a><span class="s2">X_pca = pca.fit(X_std)</span>
<a name="l484"><span class="ln">484  </span></a><span class="s2">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span>
<a name="l485"><span class="ln">485  </span></a><span class="s2">plt.xlabel(</span><span class="s3">'number of components'</span><span class="s2">)</span>
<a name="l486"><span class="ln">486  </span></a><span class="s2">plt.ylabel(</span><span class="s3">'cumulative explained variance'</span><span class="s2">)</span>
<a name="l487"><span class="ln">487  </span></a><span class="s2">plt.title(</span><span class="s3">'Explained Variance vs Number of Components, California'</span><span class="s2">)</span>
<a name="l488"><span class="ln">488  </span></a><span class="s2">plt.savefig(</span><span class="s3">'output/graph/pca_ca.png'</span><span class="s2">)</span>
<a name="l489"><span class="ln">489  </span></a><span class="s2">pca = PCA(n_components=</span><span class="s4">0.95</span><span class="s2">)</span>
<a name="l490"><span class="ln">490  </span></a><span class="s2">X_pca = pca.fit_transform(X_std)  </span><span class="s0"># this will fit and reduce dimensions</span>
<a name="l491"><span class="ln">491  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Selected Components:&quot;</span><span class="s2">, pca.n_components_)</span>
<a name="l492"><span class="ln">492  </span></a><span class="s0">#%% 
<a name="l493"><span class="ln">493  </span></a></span><span class="s2">n_pcs = pca.n_components_  </span><span class="s0"># get number of component</span>
<a name="l494"><span class="ln">494  </span></a><span class="s0"># get the index of the most important feature on EACH component</span>
<a name="l495"><span class="ln">495  </span></a><span class="s2">most_important = [np.abs(pca.components_[i]).argmax() </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l496"><span class="ln">496  </span></a><span class="s2">initial_feature_names = ca.columns</span>
<a name="l497"><span class="ln">497  </span></a><span class="s0"># get the most important feature names</span>
<a name="l498"><span class="ln">498  </span></a><span class="s2">most_important_names_ca = [initial_feature_names[most_important[i]] </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l499"><span class="ln">499  </span></a><span class="s0">#%% 
<a name="l500"><span class="ln">500  </span></a></span><span class="s2">X = me.iloc[:, </span><span class="s4">1</span><span class="s2">:].fillna(</span><span class="s4">0</span><span class="s2">).values  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l501"><span class="ln">501  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l502"><span class="ln">502  </span></a><span class="s2">X_std = sc.fit_transform(X)  </span><span class="s0"># standardizing the data</span>
<a name="l503"><span class="ln">503  </span></a><span class="s2">pca = PCA()</span>
<a name="l504"><span class="ln">504  </span></a><span class="s2">X_pca = pca.fit(X_std)</span>
<a name="l505"><span class="ln">505  </span></a><span class="s2">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span>
<a name="l506"><span class="ln">506  </span></a><span class="s2">plt.xlabel(</span><span class="s3">'number of components'</span><span class="s2">)</span>
<a name="l507"><span class="ln">507  </span></a><span class="s2">plt.ylabel(</span><span class="s3">'cumulative explained variance'</span><span class="s2">);</span>
<a name="l508"><span class="ln">508  </span></a><span class="s2">plt.title(</span><span class="s3">'Explained Variance vs Number of Components, Maine'</span><span class="s2">)</span>
<a name="l509"><span class="ln">509  </span></a><span class="s2">plt.savefig(</span><span class="s3">'output/graph/pca_me.png'</span><span class="s2">)</span>
<a name="l510"><span class="ln">510  </span></a><span class="s2">pca = PCA(n_components=</span><span class="s4">0.95</span><span class="s2">)</span>
<a name="l511"><span class="ln">511  </span></a><span class="s2">X_pca = pca.fit_transform(X_std)  </span><span class="s0"># this will fit and reduce dimensions</span>
<a name="l512"><span class="ln">512  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Selected Components:&quot;</span><span class="s2">, pca.n_components_)</span>
<a name="l513"><span class="ln">513  </span></a><span class="s0">#%% 
<a name="l514"><span class="ln">514  </span></a></span><span class="s2">n_pcs = pca.n_components_  </span><span class="s0"># get number of component</span>
<a name="l515"><span class="ln">515  </span></a><span class="s0"># get the index of the most important feature on EACH component</span>
<a name="l516"><span class="ln">516  </span></a><span class="s2">most_important = [np.abs(pca.components_[i]).argmax() </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l517"><span class="ln">517  </span></a><span class="s2">initial_feature_names = me.columns</span>
<a name="l518"><span class="ln">518  </span></a><span class="s0"># get the most important feature names</span>
<a name="l519"><span class="ln">519  </span></a><span class="s2">most_important_names_me = [initial_feature_names[most_important[i]] </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l520"><span class="ln">520  </span></a><span class="s0">#%% 
<a name="l521"><span class="ln">521  </span></a></span><span class="s2">X = oh.iloc[:, </span><span class="s4">1</span><span class="s2">:].fillna(</span><span class="s4">0</span><span class="s2">).values  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l522"><span class="ln">522  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l523"><span class="ln">523  </span></a><span class="s2">X_std = sc.fit_transform(X)  </span><span class="s0"># standardizing the data</span>
<a name="l524"><span class="ln">524  </span></a><span class="s2">pca = PCA()</span>
<a name="l525"><span class="ln">525  </span></a><span class="s2">X_pca = pca.fit(X_std)</span>
<a name="l526"><span class="ln">526  </span></a><span class="s2">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span>
<a name="l527"><span class="ln">527  </span></a><span class="s2">plt.xlabel(</span><span class="s3">'number of components'</span><span class="s2">)</span>
<a name="l528"><span class="ln">528  </span></a><span class="s2">plt.ylabel(</span><span class="s3">'cumulative explained variance'</span><span class="s2">);</span>
<a name="l529"><span class="ln">529  </span></a><span class="s2">plt.title(</span><span class="s3">'Explained Variance vs Number of Components, Ohio'</span><span class="s2">)</span>
<a name="l530"><span class="ln">530  </span></a><span class="s2">plt.savefig(</span><span class="s3">'output/graph/pca_oh.png'</span><span class="s2">)</span>
<a name="l531"><span class="ln">531  </span></a><span class="s2">pca = PCA(n_components=</span><span class="s4">0.95</span><span class="s2">)</span>
<a name="l532"><span class="ln">532  </span></a><span class="s2">X_pca = pca.fit_transform(X_std)  </span><span class="s0"># this will fit and reduce dimensions</span>
<a name="l533"><span class="ln">533  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Selected Components:&quot;</span><span class="s2">, pca.n_components_)</span>
<a name="l534"><span class="ln">534  </span></a><span class="s0">#%% 
<a name="l535"><span class="ln">535  </span></a></span><span class="s2">n_pcs = pca.n_components_  </span><span class="s0"># get number of component</span>
<a name="l536"><span class="ln">536  </span></a><span class="s0"># get the index of the most important feature on EACH component</span>
<a name="l537"><span class="ln">537  </span></a><span class="s2">most_important = [np.abs(pca.components_[i]).argmax() </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l538"><span class="ln">538  </span></a><span class="s2">initial_feature_names = oh.columns</span>
<a name="l539"><span class="ln">539  </span></a><span class="s0"># get the most important feature names</span>
<a name="l540"><span class="ln">540  </span></a><span class="s2">most_important_names_oh = [initial_feature_names[most_important[i]] </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l541"><span class="ln">541  </span></a><span class="s0">#%% 
<a name="l542"><span class="ln">542  </span></a></span><span class="s2">X = pa.iloc[:, </span><span class="s4">1</span><span class="s2">:].fillna(</span><span class="s4">0</span><span class="s2">).values  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l543"><span class="ln">543  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l544"><span class="ln">544  </span></a><span class="s2">X_std = sc.fit_transform(X)  </span><span class="s0"># standardizing the data</span>
<a name="l545"><span class="ln">545  </span></a><span class="s2">pca = PCA()</span>
<a name="l546"><span class="ln">546  </span></a><span class="s2">X_pca = pca.fit(X_std)</span>
<a name="l547"><span class="ln">547  </span></a><span class="s2">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span>
<a name="l548"><span class="ln">548  </span></a><span class="s2">plt.xlabel(</span><span class="s3">'number of components'</span><span class="s2">)</span>
<a name="l549"><span class="ln">549  </span></a><span class="s2">plt.ylabel(</span><span class="s3">'cumulative explained variance'</span><span class="s2">);</span>
<a name="l550"><span class="ln">550  </span></a><span class="s2">plt.title(</span><span class="s3">'Explained Variance vs Number of Components, Pennsylvania'</span><span class="s2">)</span>
<a name="l551"><span class="ln">551  </span></a><span class="s2">plt.savefig(</span><span class="s3">'output/graph/pca_pa.png'</span><span class="s2">)</span>
<a name="l552"><span class="ln">552  </span></a><span class="s2">pca = PCA(n_components=</span><span class="s4">0.95</span><span class="s2">)</span>
<a name="l553"><span class="ln">553  </span></a><span class="s2">X_pca = pca.fit_transform(X_std)  </span><span class="s0"># this will fit and reduce dimensions</span>
<a name="l554"><span class="ln">554  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Selected Components:&quot;</span><span class="s2">, pca.n_components_)</span>
<a name="l555"><span class="ln">555  </span></a><span class="s0">#%% 
<a name="l556"><span class="ln">556  </span></a></span><span class="s2">n_pcs = pca.n_components_  </span><span class="s0"># get number of component</span>
<a name="l557"><span class="ln">557  </span></a><span class="s0"># get the index of the most important feature on EACH component</span>
<a name="l558"><span class="ln">558  </span></a><span class="s2">most_important = [np.abs(pca.components_[i]).argmax() </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l559"><span class="ln">559  </span></a><span class="s2">initial_feature_names = pa.columns</span>
<a name="l560"><span class="ln">560  </span></a><span class="s0"># get the most important feature names</span>
<a name="l561"><span class="ln">561  </span></a><span class="s2">most_important_names_pa = [initial_feature_names[most_important[i]] </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l562"><span class="ln">562  </span></a><span class="s0">#%% 
<a name="l563"><span class="ln">563  </span></a></span><span class="s2">X = sa.iloc[:, </span><span class="s4">1</span><span class="s2">:].fillna(</span><span class="s4">0</span><span class="s2">).values  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l564"><span class="ln">564  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l565"><span class="ln">565  </span></a><span class="s2">X_std = sc.fit_transform(X)  </span><span class="s0"># standardizing the data</span>
<a name="l566"><span class="ln">566  </span></a><span class="s2">pca = PCA()</span>
<a name="l567"><span class="ln">567  </span></a><span class="s2">X_pca = pca.fit(X_std)</span>
<a name="l568"><span class="ln">568  </span></a><span class="s2">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span>
<a name="l569"><span class="ln">569  </span></a><span class="s2">plt.xlabel(</span><span class="s3">'number of components'</span><span class="s2">)</span>
<a name="l570"><span class="ln">570  </span></a><span class="s2">plt.ylabel(</span><span class="s3">'cumulative explained variance'</span><span class="s2">);</span>
<a name="l571"><span class="ln">571  </span></a><span class="s2">plt.title(</span><span class="s3">'Explained Variance vs Number of Components, South Carolina'</span><span class="s2">)</span>
<a name="l572"><span class="ln">572  </span></a><span class="s2">plt.savefig(</span><span class="s3">'output/graph/pca_sc.png'</span><span class="s2">)</span>
<a name="l573"><span class="ln">573  </span></a><span class="s2">pca = PCA(n_components=</span><span class="s4">0.95</span><span class="s2">)</span>
<a name="l574"><span class="ln">574  </span></a><span class="s2">X_pca = pca.fit_transform(X_std)  </span><span class="s0"># this will fit and reduce dimensions</span>
<a name="l575"><span class="ln">575  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Selected Components:&quot;</span><span class="s2">, pca.n_components_)</span>
<a name="l576"><span class="ln">576  </span></a><span class="s0">#%% 
<a name="l577"><span class="ln">577  </span></a></span><span class="s2">n_pcs = pca.n_components_  </span><span class="s0"># get number of component</span>
<a name="l578"><span class="ln">578  </span></a><span class="s0"># get the index of the most important feature on EACH component</span>
<a name="l579"><span class="ln">579  </span></a><span class="s2">most_important = [np.abs(pca.components_[i]).argmax() </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l580"><span class="ln">580  </span></a><span class="s2">initial_feature_names = sa.columns</span>
<a name="l581"><span class="ln">581  </span></a><span class="s0"># get the most important feature names</span>
<a name="l582"><span class="ln">582  </span></a><span class="s2">most_important_names_sc = [initial_feature_names[most_important[i]] </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l583"><span class="ln">583  </span></a><span class="s0">#%% 
<a name="l584"><span class="ln">584  </span></a></span><span class="s2">X = md.iloc[:, </span><span class="s4">1</span><span class="s2">:].fillna(</span><span class="s4">0</span><span class="s2">).values  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l585"><span class="ln">585  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l586"><span class="ln">586  </span></a><span class="s2">X_std = sc.fit_transform(X)  </span><span class="s0"># standardizing the data</span>
<a name="l587"><span class="ln">587  </span></a><span class="s2">pca = PCA()</span>
<a name="l588"><span class="ln">588  </span></a><span class="s2">X_pca = pca.fit(X_std)</span>
<a name="l589"><span class="ln">589  </span></a><span class="s2">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span>
<a name="l590"><span class="ln">590  </span></a><span class="s2">plt.xlabel(</span><span class="s3">'number of components'</span><span class="s2">)</span>
<a name="l591"><span class="ln">591  </span></a><span class="s2">plt.ylabel(</span><span class="s3">'cumulative explained variance'</span><span class="s2">);</span>
<a name="l592"><span class="ln">592  </span></a><span class="s2">plt.title(</span><span class="s3">'Explained Variance vs Number of Components, Maryland'</span><span class="s2">)</span>
<a name="l593"><span class="ln">593  </span></a><span class="s2">plt.savefig(</span><span class="s3">'output/graph/pca_md.png'</span><span class="s2">)</span>
<a name="l594"><span class="ln">594  </span></a><span class="s2">pca = PCA(n_components=</span><span class="s4">0.95</span><span class="s2">)</span>
<a name="l595"><span class="ln">595  </span></a><span class="s2">X_pca = pca.fit_transform(X_std)  </span><span class="s0"># this will fit and reduce dimensions</span>
<a name="l596"><span class="ln">596  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Selected Components:&quot;</span><span class="s2">, pca.n_components_)</span>
<a name="l597"><span class="ln">597  </span></a><span class="s0">#%% 
<a name="l598"><span class="ln">598  </span></a></span><span class="s2">n_pcs = pca.n_components_  </span><span class="s0"># get number of component</span>
<a name="l599"><span class="ln">599  </span></a><span class="s0"># get the index of the most important feature on EACH component</span>
<a name="l600"><span class="ln">600  </span></a><span class="s2">most_important = [np.abs(pca.components_[i]).argmax() </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l601"><span class="ln">601  </span></a><span class="s2">initial_feature_names = md.columns</span>
<a name="l602"><span class="ln">602  </span></a><span class="s0"># get the most important feature names</span>
<a name="l603"><span class="ln">603  </span></a><span class="s2">most_important_names_md = [initial_feature_names[most_important[i]] </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l604"><span class="ln">604  </span></a><span class="s0">#%% 
<a name="l605"><span class="ln">605  </span></a></span><span class="s2">X = ny.iloc[:, </span><span class="s4">1</span><span class="s2">:].fillna(</span><span class="s4">0</span><span class="s2">).values  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l606"><span class="ln">606  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l607"><span class="ln">607  </span></a><span class="s2">X_std = sc.fit_transform(X)  </span><span class="s0"># standardizing the data</span>
<a name="l608"><span class="ln">608  </span></a><span class="s2">pca = PCA()</span>
<a name="l609"><span class="ln">609  </span></a><span class="s2">X_pca = pca.fit(X_std)</span>
<a name="l610"><span class="ln">610  </span></a><span class="s2">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span>
<a name="l611"><span class="ln">611  </span></a><span class="s2">plt.xlabel(</span><span class="s3">'number of components'</span><span class="s2">)</span>
<a name="l612"><span class="ln">612  </span></a><span class="s2">plt.ylabel(</span><span class="s3">'cumulative explained variance'</span><span class="s2">);</span>
<a name="l613"><span class="ln">613  </span></a><span class="s2">plt.title(</span><span class="s3">'Explained Variance vs Number of Components, New York'</span><span class="s2">)</span>
<a name="l614"><span class="ln">614  </span></a><span class="s2">plt.savefig(</span><span class="s3">'output/graph/pca_ny.png'</span><span class="s2">)</span>
<a name="l615"><span class="ln">615  </span></a><span class="s2">pca = PCA(n_components=</span><span class="s4">0.95</span><span class="s2">)</span>
<a name="l616"><span class="ln">616  </span></a><span class="s2">X_pca = pca.fit_transform(X_std)  </span><span class="s0"># this will fit and reduce dimensions</span>
<a name="l617"><span class="ln">617  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Selected Components:&quot;</span><span class="s2">, pca.n_components_)</span>
<a name="l618"><span class="ln">618  </span></a><span class="s0">#%% 
<a name="l619"><span class="ln">619  </span></a></span><span class="s2">n_pcs = pca.n_components_  </span><span class="s0"># get number of component</span>
<a name="l620"><span class="ln">620  </span></a><span class="s0"># get the index of the most important feature on EACH component</span>
<a name="l621"><span class="ln">621  </span></a><span class="s2">most_important = [np.abs(pca.components_[i]).argmax() </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l622"><span class="ln">622  </span></a><span class="s2">initial_feature_names = ny.columns</span>
<a name="l623"><span class="ln">623  </span></a><span class="s0"># get the most important feature names</span>
<a name="l624"><span class="ln">624  </span></a><span class="s2">most_important_names_ny = [initial_feature_names[most_important[i]] </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l625"><span class="ln">625  </span></a><span class="s0">#%% 
<a name="l626"><span class="ln">626  </span></a></span><span class="s2">print(</span><span class="s3">&quot;PCA Features for Ohio:&quot;</span><span class="s2">, most_important_names_oh)</span>
<a name="l627"><span class="ln">627  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Features for Pennsylvania:&quot;</span><span class="s2">, most_important_names_pa)</span>
<a name="l628"><span class="ln">628  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Features for South Carolina:&quot;</span><span class="s2">, most_important_names_sc)</span>
<a name="l629"><span class="ln">629  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Features for Maryland:&quot;</span><span class="s2">, most_important_names_md)</span>
<a name="l630"><span class="ln">630  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Features for New York:&quot;</span><span class="s2">, most_important_names_ny)</span>
<a name="l631"><span class="ln">631  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Features for California:&quot;</span><span class="s2">, most_important_names_ca)</span>
<a name="l632"><span class="ln">632  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Features for Maine:&quot;</span><span class="s2">, most_important_names_me)</span>
<a name="l633"><span class="ln">633  </span></a><span class="s0">#%% 
<a name="l634"><span class="ln">634  </span></a></span><span class="s2">list(set(most_important_names_oh) &amp; set(most_important_names_pa) &amp; set(most_important_names_sc) &amp; set(</span>
<a name="l635"><span class="ln">635  </span></a>    <span class="s2">most_important_names_md) &amp; set(most_important_names_ny) &amp; set(most_important_names_ca) &amp; set(</span>
<a name="l636"><span class="ln">636  </span></a>    <span class="s2">most_important_names_me))</span>
<a name="l637"><span class="ln">637  </span></a><span class="s0">#%% md 
<a name="l638"><span class="ln">638  </span></a></span><span class="s2">### Linear Regression 
<a name="l639"><span class="ln">639  </span></a></span><span class="s0">#%% 
<a name="l640"><span class="ln">640  </span></a></span><span class="s1">from </span><span class="s2">sklearn.metrics </span><span class="s1">import </span><span class="s2">r2_score, mean_squared_error</span>
<a name="l641"><span class="ln">641  </span></a><span class="s1">from </span><span class="s2">sklearn.model_selection </span><span class="s1">import </span><span class="s2">train_test_split</span>
<a name="l642"><span class="ln">642  </span></a><span class="s1">from </span><span class="s2">sklearn.linear_model </span><span class="s1">import </span><span class="s2">LinearRegression</span>
<a name="l643"><span class="ln">643  </span></a>
<a name="l644"><span class="ln">644  </span></a><span class="s2">X</span>
<a name="l645"><span class="ln">645  </span></a><span class="s0">#%% 
<a name="l646"><span class="ln">646  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l647"><span class="ln">647  </span></a><span class="s2">y = md[</span><span class="s3">'deaths_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l648"><span class="ln">648  </span></a><span class="s2">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s2">, random_state=</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># splitting the data</span>
<a name="l649"><span class="ln">649  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l650"><span class="ln">650  </span></a><span class="s2">X_train_std = sc.fit_transform(X_train)  </span><span class="s0"># standardizing the data</span>
<a name="l651"><span class="ln">651  </span></a><span class="s2">X_test_std = sc.transform(X_test)  </span><span class="s0"># standardizing the data</span>
<a name="l652"><span class="ln">652  </span></a><span class="s2">lr = LinearRegression()  </span><span class="s0"># creating a LinearRegression object</span>
<a name="l653"><span class="ln">653  </span></a><span class="s2">lr.fit(X_train_std, y_train)  </span><span class="s0"># fitting the training data</span>
<a name="l654"><span class="ln">654  </span></a><span class="s2">y_pred = lr.predict(X_test_std)  </span><span class="s0"># predicting the test data</span>
<a name="l655"><span class="ln">655  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y_test, y_pred))</span>
<a name="l656"><span class="ln">656  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y_test, y_pred))</span>
<a name="l657"><span class="ln">657  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<a name="l658"><span class="ln">658  </span></a><span class="s0">#%% 
<a name="l659"><span class="ln">659  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l660"><span class="ln">660  </span></a><span class="s2">y = md[</span><span class="s3">'deaths'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l661"><span class="ln">661  </span></a><span class="s2">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s2">, random_state=</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># splitting the data</span>
<a name="l662"><span class="ln">662  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l663"><span class="ln">663  </span></a><span class="s2">X_train_std = sc.fit_transform(X_train)  </span><span class="s0"># standardizing the data</span>
<a name="l664"><span class="ln">664  </span></a><span class="s2">X_test_std = sc.transform(X_test)  </span><span class="s0"># standardizing the data</span>
<a name="l665"><span class="ln">665  </span></a><span class="s2">lr = LinearRegression()  </span><span class="s0"># creating a LinearRegression object</span>
<a name="l666"><span class="ln">666  </span></a><span class="s2">lr.fit(X_train_std, y_train)  </span><span class="s0"># fitting the training data</span>
<a name="l667"><span class="ln">667  </span></a><span class="s2">y_pred = lr.predict(X_test_std)  </span><span class="s0"># predicting the test data</span>
<a name="l668"><span class="ln">668  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y_test, y_pred))</span>
<a name="l669"><span class="ln">669  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y_test, y_pred))</span>
<a name="l670"><span class="ln">670  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<a name="l671"><span class="ln">671  </span></a><span class="s0">#%% 
<a name="l672"><span class="ln">672  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l673"><span class="ln">673  </span></a><span class="s2">y = md[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l674"><span class="ln">674  </span></a><span class="s2">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s2">, random_state=</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># splitting the data</span>
<a name="l675"><span class="ln">675  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l676"><span class="ln">676  </span></a><span class="s2">X_train_std = sc.fit_transform(X_train)  </span><span class="s0"># standardizing the data</span>
<a name="l677"><span class="ln">677  </span></a><span class="s2">X_test_std = sc.transform(X_test)  </span><span class="s0"># standardizing the data</span>
<a name="l678"><span class="ln">678  </span></a><span class="s2">lr = LinearRegression()  </span><span class="s0"># creating a LinearRegression object</span>
<a name="l679"><span class="ln">679  </span></a><span class="s2">lr.fit(X_train_std, y_train)  </span><span class="s0"># fitting the training data</span>
<a name="l680"><span class="ln">680  </span></a><span class="s2">y_pred = lr.predict(X_test_std)  </span><span class="s0"># predicting the test data</span>
<a name="l681"><span class="ln">681  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y_test, y_pred))</span>
<a name="l682"><span class="ln">682  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y_test, y_pred))</span>
<a name="l683"><span class="ln">683  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<a name="l684"><span class="ln">684  </span></a><span class="s0">#%% 
<a name="l685"><span class="ln">685  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l686"><span class="ln">686  </span></a><span class="s2">y = md[</span><span class="s3">'cases'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l687"><span class="ln">687  </span></a><span class="s2">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s2">, random_state=</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># splitting the data</span>
<a name="l688"><span class="ln">688  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l689"><span class="ln">689  </span></a><span class="s2">X_train_std = sc.fit_transform(X_train)  </span><span class="s0"># standardizing the data</span>
<a name="l690"><span class="ln">690  </span></a><span class="s2">X_test_std = sc.transform(X_test)  </span><span class="s0"># standardizing the data</span>
<a name="l691"><span class="ln">691  </span></a><span class="s2">lr = LinearRegression()  </span><span class="s0"># creating a LinearRegression object</span>
<a name="l692"><span class="ln">692  </span></a><span class="s2">lr.fit(X_train_std, y_train)  </span><span class="s0"># fitting the training data</span>
<a name="l693"><span class="ln">693  </span></a><span class="s2">y_pred = lr.predict(X_test_std)  </span><span class="s0"># predicting the test data</span>
<a name="l694"><span class="ln">694  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y_test, y_pred))</span>
<a name="l695"><span class="ln">695  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y_test, y_pred))</span>
<a name="l696"><span class="ln">696  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<a name="l697"><span class="ln">697  </span></a><span class="s0">#%% 
<a name="l698"><span class="ln">698  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l699"><span class="ln">699  </span></a><span class="s2">y = md[</span><span class="s3">'daily_cases_100k'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l700"><span class="ln">700  </span></a><span class="s2">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s2">, random_state=</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># splitting the data</span>
<a name="l701"><span class="ln">701  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l702"><span class="ln">702  </span></a><span class="s2">X_train_std = sc.fit_transform(X_train)  </span><span class="s0"># standardizing the data</span>
<a name="l703"><span class="ln">703  </span></a><span class="s2">X_test_std = sc.transform(X_test)  </span><span class="s0"># standardizing the data</span>
<a name="l704"><span class="ln">704  </span></a><span class="s2">lr = LinearRegression()  </span><span class="s0"># creating a LinearRegression object</span>
<a name="l705"><span class="ln">705  </span></a><span class="s2">lr.fit(X_train_std, y_train)  </span><span class="s0"># fitting the training data</span>
<a name="l706"><span class="ln">706  </span></a><span class="s2">y_pred = lr.predict(X_test_std)  </span><span class="s0"># predicting the test data</span>
<a name="l707"><span class="ln">707  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y_test, y_pred))</span>
<a name="l708"><span class="ln">708  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y_test, y_pred))</span>
<a name="l709"><span class="ln">709  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<a name="l710"><span class="ln">710  </span></a><span class="s0">#%% 
<a name="l711"><span class="ln">711  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l712"><span class="ln">712  </span></a><span class="s2">y = md[</span><span class="s3">'daily_cases'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l713"><span class="ln">713  </span></a><span class="s2">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s2">, random_state=</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># splitting the data</span>
<a name="l714"><span class="ln">714  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l715"><span class="ln">715  </span></a><span class="s2">X_train_std = sc.fit_transform(X_train)  </span><span class="s0"># standardizing the data</span>
<a name="l716"><span class="ln">716  </span></a><span class="s2">X_test_std = sc.transform(X_test)  </span><span class="s0"># standardizing the data</span>
<a name="l717"><span class="ln">717  </span></a><span class="s2">lr = LinearRegression()  </span><span class="s0"># creating a LinearRegression object</span>
<a name="l718"><span class="ln">718  </span></a><span class="s2">lr.fit(X_train_std, y_train)  </span><span class="s0"># fitting the training data</span>
<a name="l719"><span class="ln">719  </span></a><span class="s2">y_pred = lr.predict(X_test_std)  </span><span class="s0"># predicting the test data</span>
<a name="l720"><span class="ln">720  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y_test, y_pred))</span>
<a name="l721"><span class="ln">721  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y_test, y_pred))</span>
<a name="l722"><span class="ln">722  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<a name="l723"><span class="ln">723  </span></a><span class="s0">#%% 
<a name="l724"><span class="ln">724  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l725"><span class="ln">725  </span></a><span class="s2">y = md[</span><span class="s3">'daily_deaths_100k'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l726"><span class="ln">726  </span></a><span class="s2">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s2">, random_state=</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># splitting the data</span>
<a name="l727"><span class="ln">727  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l728"><span class="ln">728  </span></a><span class="s2">X_train_std = sc.fit_transform(X_train)  </span><span class="s0"># standardizing the data</span>
<a name="l729"><span class="ln">729  </span></a><span class="s2">X_test_std = sc.transform(X_test)  </span><span class="s0"># standardizing the data</span>
<a name="l730"><span class="ln">730  </span></a><span class="s2">lr = LinearRegression()  </span><span class="s0"># creating a LinearRegression object</span>
<a name="l731"><span class="ln">731  </span></a><span class="s2">lr.fit(X_train_std, y_train)  </span><span class="s0"># fitting the training data</span>
<a name="l732"><span class="ln">732  </span></a><span class="s2">y_pred = lr.predict(X_test_std)  </span><span class="s0"># predicting the test data</span>
<a name="l733"><span class="ln">733  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y_test, y_pred))</span>
<a name="l734"><span class="ln">734  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y_test, y_pred))</span>
<a name="l735"><span class="ln">735  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<a name="l736"><span class="ln">736  </span></a><span class="s0">#%% 
<a name="l737"><span class="ln">737  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l738"><span class="ln">738  </span></a><span class="s2">y = md[</span><span class="s3">'daily_deaths'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l739"><span class="ln">739  </span></a><span class="s2">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s2">, random_state=</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># splitting the data</span>
<a name="l740"><span class="ln">740  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l741"><span class="ln">741  </span></a><span class="s2">X_train_std = sc.fit_transform(X_train)  </span><span class="s0"># standardizing the data</span>
<a name="l742"><span class="ln">742  </span></a><span class="s2">X_test_std = sc.transform(X_test)  </span><span class="s0"># standardizing the data</span>
<a name="l743"><span class="ln">743  </span></a><span class="s2">lr = LinearRegression()  </span><span class="s0"># creating a LinearRegression object</span>
<a name="l744"><span class="ln">744  </span></a><span class="s2">lr.fit(X_train_std, y_train)  </span><span class="s0"># fitting the training data</span>
<a name="l745"><span class="ln">745  </span></a><span class="s2">y_pred = lr.predict(X_test_std)  </span><span class="s0"># predicting the test data</span>
<a name="l746"><span class="ln">746  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y_test, y_pred))</span>
<a name="l747"><span class="ln">747  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y_test, y_pred))</span>
<a name="l748"><span class="ln">748  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y_test, y_pred)))</span>
<a name="l749"><span class="ln">749  </span></a><span class="s0">#%% 
<a name="l750"><span class="ln">750  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l751"><span class="ln">751  </span></a><span class="s2">y = md[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l752"><span class="ln">752  </span></a><span class="s2">X = sm.add_constant(X)  </span><span class="s0"># adding a constant</span>
<a name="l753"><span class="ln">753  </span></a><span class="s2">model_md_cases = sm.OLS(y, X).fit()</span>
<a name="l754"><span class="ln">754  </span></a><span class="s2">print(model_md_cases.summary())</span>
<a name="l755"><span class="ln">755  </span></a><span class="s0">#%% 
<a name="l756"><span class="ln">756  </span></a></span><span class="s2">X = md[most_important_names_md].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l757"><span class="ln">757  </span></a><span class="s2">y = md[</span><span class="s3">'deaths_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l758"><span class="ln">758  </span></a><span class="s2">X = sm.add_constant(X)  </span><span class="s0"># adding a constant</span>
<a name="l759"><span class="ln">759  </span></a><span class="s2">model_md_deathes = sm.OLS(y, X).fit()</span>
<a name="l760"><span class="ln">760  </span></a><span class="s2">print(model_md_deathes.summary())</span>
<a name="l761"><span class="ln">761  </span></a><span class="s0">#%% 
<a name="l762"><span class="ln">762  </span></a></span><span class="s2">X = ca[most_important_names_ca].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l763"><span class="ln">763  </span></a><span class="s2">y = ca[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l764"><span class="ln">764  </span></a><span class="s2">X = sm.add_constant(X)  </span><span class="s0"># adding a constant</span>
<a name="l765"><span class="ln">765  </span></a><span class="s2">model_ca_cases = sm.OLS(y, X).fit()</span>
<a name="l766"><span class="ln">766  </span></a><span class="s2">print(model_ca_cases.summary())</span>
<a name="l767"><span class="ln">767  </span></a><span class="s2">y = ca[</span><span class="s3">'deaths_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l768"><span class="ln">768  </span></a><span class="s2">model_ca_deathes = sm.OLS(y, X).fit()</span>
<a name="l769"><span class="ln">769  </span></a><span class="s2">print(model_ca_deathes.summary())</span>
<a name="l770"><span class="ln">770  </span></a><span class="s0">#%% 
<a name="l771"><span class="ln">771  </span></a></span><span class="s2">X = oh[most_important_names_oh].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l772"><span class="ln">772  </span></a><span class="s2">y = oh[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l773"><span class="ln">773  </span></a><span class="s2">X = sm.add_constant(X)  </span><span class="s0"># adding a constant</span>
<a name="l774"><span class="ln">774  </span></a><span class="s2">model_oh_cases = sm.OLS(y, X).fit()</span>
<a name="l775"><span class="ln">775  </span></a><span class="s2">print(model_oh_cases.summary())</span>
<a name="l776"><span class="ln">776  </span></a><span class="s2">y = oh[</span><span class="s3">'deaths_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l777"><span class="ln">777  </span></a><span class="s2">model_oh_deathes = sm.OLS(y, X).fit()</span>
<a name="l778"><span class="ln">778  </span></a><span class="s2">print(model_oh_deathes.summary())</span>
<a name="l779"><span class="ln">779  </span></a><span class="s0">#%% 
<a name="l780"><span class="ln">780  </span></a></span><span class="s2">X = ny[most_important_names_ny].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l781"><span class="ln">781  </span></a><span class="s2">y = ny[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l782"><span class="ln">782  </span></a><span class="s2">X = sm.add_constant(X)  </span><span class="s0"># adding a constant</span>
<a name="l783"><span class="ln">783  </span></a><span class="s2">model_ny_cases = sm.OLS(y, X).fit()</span>
<a name="l784"><span class="ln">784  </span></a><span class="s2">print(model_ny_cases.summary())</span>
<a name="l785"><span class="ln">785  </span></a><span class="s2">y = ny[</span><span class="s3">'deaths_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l786"><span class="ln">786  </span></a><span class="s2">model_ny_deathes = sm.OLS(y, X).fit()</span>
<a name="l787"><span class="ln">787  </span></a><span class="s2">print(model_ny_deathes.summary())</span>
<a name="l788"><span class="ln">788  </span></a><span class="s0">#%% 
<a name="l789"><span class="ln">789  </span></a></span><span class="s2">X = me[most_important_names_me].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l790"><span class="ln">790  </span></a><span class="s2">y = me[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l791"><span class="ln">791  </span></a><span class="s2">X = sm.add_constant(X)  </span><span class="s0"># adding a constant</span>
<a name="l792"><span class="ln">792  </span></a><span class="s2">model_me_cases = sm.OLS(y, X).fit()</span>
<a name="l793"><span class="ln">793  </span></a><span class="s2">print(model_me_cases.summary())</span>
<a name="l794"><span class="ln">794  </span></a><span class="s2">y = me[</span><span class="s3">'deaths_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l795"><span class="ln">795  </span></a><span class="s2">model_me_deathes = sm.OLS(y, X).fit()</span>
<a name="l796"><span class="ln">796  </span></a><span class="s2">print(model_me_deathes.summary())</span>
<a name="l797"><span class="ln">797  </span></a><span class="s0">#%% 
<a name="l798"><span class="ln">798  </span></a></span><span class="s2">X = pa[most_important_names_pa].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l799"><span class="ln">799  </span></a><span class="s2">y = pa[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l800"><span class="ln">800  </span></a><span class="s2">X = sm.add_constant(X)  </span><span class="s0"># adding a constant</span>
<a name="l801"><span class="ln">801  </span></a><span class="s2">model_pa_cases = sm.OLS(y, X).fit()</span>
<a name="l802"><span class="ln">802  </span></a><span class="s2">print(model_pa_cases.summary())</span>
<a name="l803"><span class="ln">803  </span></a><span class="s2">y = pa[</span><span class="s3">'deaths_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l804"><span class="ln">804  </span></a><span class="s2">model_pa_deathes = sm.OLS(y, X).fit()</span>
<a name="l805"><span class="ln">805  </span></a><span class="s2">print(model_pa_deathes.summary())</span>
<a name="l806"><span class="ln">806  </span></a><span class="s0">#%% 
<a name="l807"><span class="ln">807  </span></a></span><span class="s2">X = sa[most_important_names_sc].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l808"><span class="ln">808  </span></a><span class="s2">y = sa[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l809"><span class="ln">809  </span></a><span class="s2">X = sm.add_constant(X)  </span><span class="s0"># adding a constant</span>
<a name="l810"><span class="ln">810  </span></a><span class="s2">model_sa_cases = sm.OLS(y, X).fit()</span>
<a name="l811"><span class="ln">811  </span></a><span class="s2">print(model_sa_cases.summary())</span>
<a name="l812"><span class="ln">812  </span></a><span class="s2">y = sa[</span><span class="s3">'deaths_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l813"><span class="ln">813  </span></a><span class="s2">model_sa_deathes = sm.OLS(y, X).fit()</span>
<a name="l814"><span class="ln">814  </span></a><span class="s2">print(model_sa_deathes.summary())</span>
<a name="l815"><span class="ln">815  </span></a><span class="s0">#%% 
<a name="l816"><span class="ln">816  </span></a></span><span class="s2">X = df[most_important_names_sc].fillna(</span><span class="s4">0</span><span class="s2">)  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l817"><span class="ln">817  </span></a><span class="s2">y = df[</span><span class="s3">'cases_transformed'</span><span class="s2">]  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l818"><span class="ln">818  </span></a><span class="s2">y_pred = model_sa_cases.predict(X)</span>
<a name="l819"><span class="ln">819  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression R2 Score:&quot;</span><span class="s2">, r2_score(y, y_pred))</span>
<a name="l820"><span class="ln">820  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression MSE:&quot;</span><span class="s2">, mean_squared_error(y, y_pred))</span>
<a name="l821"><span class="ln">821  </span></a><span class="s2">print(</span><span class="s3">&quot;Linear Regression RMSE:&quot;</span><span class="s2">, np.sqrt(mean_squared_error(y, y_pred)))</span>
<a name="l822"><span class="ln">822  </span></a><span class="s0">#%% 
<a name="l823"><span class="ln">823  </span></a></span><span class="s2">seven_state = pd.concat(</span>
<a name="l824"><span class="ln">824  </span></a>    <span class="s2">[df, ca, oh, ny, me, pa, sa, md], axis=</span><span class="s4">0</span><span class="s2">, ignore_index=</span><span class="s1">True</span><span class="s2">, sort=</span><span class="s1">False</span><span class="s2">)</span>
<a name="l825"><span class="ln">825  </span></a><span class="s2">seven_state = seven_state.reset_index(drop=</span><span class="s1">True</span><span class="s2">)</span>
<a name="l826"><span class="ln">826  </span></a><span class="s0">#%% 
<a name="l827"><span class="ln">827  </span></a></span><span class="s1">from </span><span class="s2">sklearn </span><span class="s1">import </span><span class="s2">svm</span>
<a name="l828"><span class="ln">828  </span></a><span class="s1">from </span><span class="s2">sklearn.tree </span><span class="s1">import </span><span class="s2">DecisionTreeClassifier</span>
<a name="l829"><span class="ln">829  </span></a><span class="s1">from </span><span class="s2">sklearn.model_selection </span><span class="s1">import </span><span class="s2">KFold, cross_val_score</span>
<a name="l830"><span class="ln">830  </span></a><span class="s0">#%% 
<a name="l831"><span class="ln">831  </span></a></span><span class="s2">X = seven_state.iloc[:, </span><span class="s4">1</span><span class="s2">:].fillna(</span><span class="s4">0</span><span class="s2">).values  </span><span class="s0"># getting all values as a matrix of dataframe</span>
<a name="l832"><span class="ln">832  </span></a><span class="s2">sc = StandardScaler()  </span><span class="s0"># creating a StandardScaler object</span>
<a name="l833"><span class="ln">833  </span></a><span class="s2">X_std = sc.fit_transform(X)  </span><span class="s0"># standardizing the data</span>
<a name="l834"><span class="ln">834  </span></a><span class="s2">pca = PCA()</span>
<a name="l835"><span class="ln">835  </span></a><span class="s2">X_pca = pca.fit(X_std)</span>
<a name="l836"><span class="ln">836  </span></a><span class="s2">plt.plot(np.cumsum(pca.explained_variance_ratio_))</span>
<a name="l837"><span class="ln">837  </span></a><span class="s2">plt.xlabel(</span><span class="s3">'number of components'</span><span class="s2">)</span>
<a name="l838"><span class="ln">838  </span></a><span class="s2">plt.ylabel(</span><span class="s3">'cumulative explained variance'</span><span class="s2">);</span>
<a name="l839"><span class="ln">839  </span></a><span class="s2">plt.title(</span><span class="s3">'Explained Variance vs Number of Components, South Carolina'</span><span class="s2">)</span>
<a name="l840"><span class="ln">840  </span></a><span class="s2">plt.savefig(</span><span class="s3">'output/graph/pca_sc.png'</span><span class="s2">)</span>
<a name="l841"><span class="ln">841  </span></a><span class="s2">pca = PCA(n_components=</span><span class="s4">0.95</span><span class="s2">)</span>
<a name="l842"><span class="ln">842  </span></a><span class="s2">X_pca = pca.fit_transform(X_std)  </span><span class="s0"># this will fit and reduce dimensions</span>
<a name="l843"><span class="ln">843  </span></a><span class="s2">print(</span><span class="s3">&quot;PCA Selected Components:&quot;</span><span class="s2">, pca.n_components_)</span>
<a name="l844"><span class="ln">844  </span></a><span class="s0">#%% 
<a name="l845"><span class="ln">845  </span></a></span><span class="s2">n_pcs = pca.n_components_  </span><span class="s0"># get number of component</span>
<a name="l846"><span class="ln">846  </span></a><span class="s0"># get the index of the most important feature on EACH component</span>
<a name="l847"><span class="ln">847  </span></a><span class="s2">most_important = [np.abs(pca.components_[i]).argmax() </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l848"><span class="ln">848  </span></a><span class="s2">initial_feature_names = seven_state.columns</span>
<a name="l849"><span class="ln">849  </span></a><span class="s0"># get the most important feature names</span>
<a name="l850"><span class="ln">850  </span></a><span class="s2">most_important_names_seven = [initial_feature_names[most_important[i]] </span><span class="s1">for </span><span class="s2">i </span><span class="s1">in </span><span class="s2">range(n_pcs)]</span>
<a name="l851"><span class="ln">851  </span></a><span class="s0">#%% 
<a name="l852"><span class="ln">852  </span></a></span><span class="s2">most_important_names_seven</span>
<a name="l853"><span class="ln">853  </span></a><span class="s0">#%% 
<a name="l854"><span class="ln">854  </span></a></span><span class="s2">union_all = list(set(most_important_names_seven) | set(most_important_names_sc) | set(most_important_names_md) | set(</span>
<a name="l855"><span class="ln">855  </span></a>    <span class="s2">most_important_names_ca) | set(most_important_names_oh) | set(most_important_names_ny) | set(</span>
<a name="l856"><span class="ln">856  </span></a>    <span class="s2">most_important_names_me) | set(most_important_names_pa))</span>
<a name="l857"><span class="ln">857  </span></a><span class="s0">#%% 
<a name="l858"><span class="ln">858  </span></a></span><span class="s2">print(union_all)</span>
<a name="l859"><span class="ln">859  </span></a><span class="s0">#%% 
<a name="l860"><span class="ln">860  </span></a></span><span class="s2">union_seve = list(set(most_important_names_sc) | set(most_important_names_md) | set(</span>
<a name="l861"><span class="ln">861  </span></a>    <span class="s2">most_important_names_ca) | set(most_important_names_oh) | set(most_important_names_ny) | set(</span>
<a name="l862"><span class="ln">862  </span></a>    <span class="s2">most_important_names_me) | set(most_important_names_pa))</span>
<a name="l863"><span class="ln">863  </span></a><span class="s0">#%% 
<a name="l864"><span class="ln">864  </span></a></span><span class="s2">print(union_seve)</span>
<a name="l865"><span class="ln">865  </span></a><span class="s0">#%% 
<a name="l866"><span class="ln">866  </span></a></span><span class="s2">intersect = list(set(union_all) &amp; set(union_seve))</span>
<a name="l867"><span class="ln">867  </span></a><span class="s0">#%% 
<a name="l868"><span class="ln">868  </span></a></span><span class="s2">print(intersect)</span>
<a name="l869"><span class="ln">869  </span></a><span class="s0">#%% 
<a name="l870"><span class="ln">870  </span></a></span><span class="s2">len(union_all)</span>
<a name="l871"><span class="ln">871  </span></a><span class="s0">#%% 
<a name="l872"><span class="ln">872  </span></a></span><span class="s2">len(union_seve)</span>
<a name="l873"><span class="ln">873  </span></a><span class="s0">#%% 
<a name="l874"><span class="ln">874  </span></a></span><span class="s2">len(intersect)</span>
<a name="l875"><span class="ln">875  </span></a><span class="s0">#%% 
<a name="l876"><span class="ln">876  </span></a></span></pre>
</body>
</html>